{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Day 1",
   "id": "d2822c071c3bb64a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Questions\n",
    "\n",
    "1. **Easy** — What is the difference between **model training** and **model serving** in an ML system? Why are they usually separated?\n",
    "\n",
    "2. **Easy–Medium** — What is **data leakage** in machine learning? Give one concrete example from a real ML pipeline.\n",
    "\n",
    "3. **Medium** — Your model performs well offline but poorly in production. What are the **top reasons** this happens, and how would you detect them?\n",
    "\n",
    "4. **Medium–Hard** — Explain how you would design an **end-to-end ML pipeline** from data ingestion to deployment and monitoring.\n",
    "\n",
    "5. **Hard** — A deployed model’s performance slowly degrades over time, but there are no obvious bugs.\n",
    "   How would you **detect, diagnose, and fix model drift** in production?"
   ],
   "id": "a6c70ce03717741e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Answers\n",
    "1. Easy\n",
    "- Model training involves data engineering, data cleaning, feature engineer, Hyperparameter tuning, model selection where you will train the model will necessary stuff.\n",
    "- Model serving involves deploying the model to use it at production scale, it involves converting the each steps in training into python functions or scripts.\n",
    "\n",
    "**Correct Answer**<br>\n",
    "- Training is offline, batch-oriented, and compute-heavy.\n",
    "- Serving is online, latency-sensitive, stateless, and optimized for scale and reliability.\n",
    "- They are separated because they have different performance, infra, and failure requirements.\n",
    "\n",
    "**Perfect answer**\n",
    "Model training is an offline process where we clean data, engineer features, optimize model parameters using a loss function, and validate performance. It is batch-oriented and compute-heavy.\n",
    "\n",
    "Model serving is the online process of exposing a trained, versioned model through an API for real-time or batch predictions. It is latency-sensitive, scalable, and reliability-focused.\n",
    "\n",
    "They are separated because training prioritizes accuracy and experimentation, while serving prioritizes low latency, high availability, and safe rollbacks\n",
    "---\n",
    "\n",
    "2. Data leakage is a concept where training data is injected with future data that is used to evaluate the model. While training a model if you normalise the training and testing data using same mean causes data leakage.\n",
    "\n",
    "**Correct Answer**<br>\n",
    "Minor improvement: Mention label leakage or time-based leakage to sound senior.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Data leakage occurs when information that would not be available at prediction time is inadvertently used during training, leading to overly optimistic evaluation results.\n",
    "\n",
    "A common example is normalizing both train and test data using global statistics instead of fitting the scaler only on the training set. Other examples include time-based leakage or features derived from future labels.\n",
    "\n",
    "Leakage causes models to fail silently in production.\n",
    "---\n",
    "\n",
    "3. Model is trained on fixed feature but when in production there may be other factors or new features that getting involved which make the prediction wrong. Check the correlation for the features, you can communicate/collaborate with the data engineering team to identify the data related issues.\n",
    "\n",
    "4. **Answer:**\n",
    "\n",
    "The most common reasons are:\n",
    "\n",
    "* Train–serve skew: Feature computation differs between training and production.\n",
    "* Data drift: Input distributions change over time.\n",
    "* Label mismatch or delay: Production labels differ from training labels.\n",
    "* Overfitting to offline data.\n",
    "\n",
    "I would detect this by monitoring feature distributions, prediction confidence, and performance metrics over time, and by validating that the same feature pipelines are used in both training and serving.\n",
    "---\n",
    "4. To design a pipeline you need to understand the data flow\n",
    "- Data ingestion - make sure there are no anomolies, missing data, outliers that affect the model\n",
    "- Feature Engineer - Generate features from the existing features that affect the model's performance\n",
    "- Model training - training the data, saving the model in pkl file\n",
    "- Model Evaluation - Evaluating the performance of model so that it meets the performance metrics\n",
    "- Model Deployment - Deploying the model using fastapi,docker, ci/cd pipelines\n",
    "- Model Monitoring - Monitor the model for drift, inference using MLFlow\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "I’d design the pipeline as:\n",
    "\n",
    "1. Data ingestion & validation – schema checks, missing values, anomaly detection\n",
    "2. Feature engineering – reproducible transformations, ideally via a feature store\n",
    "3. Model training – experiment tracking and hyperparameter tuning\n",
    "4. Model evaluation – offline metrics and bias checks\n",
    "5. Model registry – versioned models with metadata\n",
    "6. Deployment – containerized service using FastAPI and Docker with CI/CD\n",
    "7. Monitoring – track data drift, prediction quality, latency, and errors\n",
    "\n",
    "This ensures reproducibility, scalability, and safe iteration.\n",
    "\n",
    "5. When the model's performance degrades the most likely cause is drift\n",
    "- There are two types of drift data drift and concept drift\n",
    "- First test for data drift compare mean, variance to the baseline(training data) using stastical tests KS test.\n",
    "- To detect Concept drift - Compare the distribution between data.\n",
    "\n",
    "Identify the cause of data drift - it may because of external events, change in inputs\n",
    "- To fix it you will again train the model using the latest data\n",
    "- Or you may include the features that may causing the drift\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "I’d approach drift in three steps:\n",
    "\n",
    "**Detect**\n",
    "* Monitor data drift using statistical tests like KS test on feature distributions.\n",
    "* Monitor prediction drift via confidence scores and output distributions.\n",
    "* Track performance metrics when labels are available.\n",
    "\n",
    "**Diagnose**\n",
    "* Identify which features drifted and correlate them with performance drops.\n",
    "* Check for external changes like seasonality or business logic updates.\n",
    "\n",
    "**Fix**\n",
    "* Retrain the model with recent data.\n",
    "* Add missing or more stable features.\n",
    "* Use scheduled or drift-triggered retraining.\n",
    "* This balances model freshness with operational stability."
   ],
   "id": "2eb5be29f827c63f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Day 8",
   "id": "63e63997d5c466c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Q1 (Easy — Fundamentals)**\n",
    "\n",
    "You have a dataset with **100,000 rows**, **10 numerical features**, and **1 binary target**.\n",
    "What are the **first 5 checks** you perform before training any model, and **why**?\n",
    "\n",
    "- I would check for data distribution to check the spread.\n",
    "- Any null values\n",
    "- Duplicated rows\n",
    "- Outliers\n",
    "- Correlation among features\n",
    "- These are the 5 obvious checks I would perform before training any model. Because these are the areas where models has larger impact.\n",
    "\n",
    "---\n",
    "\n",
    "### **Q2 (Easy–Medium — Data Leakage)**\n",
    "\n",
    "You are building a **customer churn model**.\n",
    "During EDA you find a feature called `last_login_date` that is **after** the churn label date for some users.\n",
    "\n",
    "* Is this data leakage?\n",
    "* How would you **detect** and **fix** it in a real pipeline?\n",
    "\n",
    "\n",
    "- No this in not data leakage. The last login date is the proof that the customer is churned.\n",
    "- I would check where the last login date is matching with churn label 1 for churned which also has last login date. And remove the column.\n",
    "---\n",
    "\n",
    "### **Q3 (Medium — Model Choice & Bias-Variance)**\n",
    "\n",
    "Your **training AUC = 0.92** and **validation AUC = 0.71**.\n",
    "\n",
    "1. What is happening?\n",
    "2. Name **3 concrete actions** you would take to fix this.\n",
    "3. Which models are **more prone** to this problem and why?\n",
    "\n",
    "---\n",
    "\n",
    "### **Q4 (Medium–Hard — Feature Engineering & Scaling)**\n",
    "\n",
    "You are using:\n",
    "\n",
    "* Logistic Regression\n",
    "* Random Forest\n",
    "* XGBoost\n",
    "\n",
    "Answer:\n",
    "\n",
    "1. Which of these **require feature scaling** and why?\n",
    "2. Would **one-hot encoding vs target encoding** change model performance? Explain **when and why**.\n",
    "3. How would you handle a categorical column with **500 unique values**?\n",
    "\n",
    "---\n",
    "\n",
    "### **Q5 (Hard — Production & ML Thinking)**\n",
    "\n",
    "Your model works well offline but **fails in production** after 2 months.\n",
    "\n",
    "* Training AUC: 0.88\n",
    "* Production AUC: 0.63\n",
    "\n",
    "1. List **4 real-world reasons** this happens.\n",
    "2. How do you **detect** the issue automatically?\n",
    "3. What **metrics or monitoring** would you put in place?\n",
    "4. When would you **retrain vs rollback**?\n",
    "\n",
    "---\n",
    "\n",
    "### **Rules**\n",
    "\n",
    "* Answer **Q1 first**.\n",
    "* I will **challenge your answers like an interviewer**.\n",
    "* Weak ML intuition → I’ll call it out.\n",
    "* Good answers → we go deeper.\n",
    "\n",
    "Start with **Q1**.\n"
   ],
   "id": "42e55c22dbd2873e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
