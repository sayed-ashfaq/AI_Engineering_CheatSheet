{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Natural Language Processing",
   "id": "61334105d03f3b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### NLP Interview Questions (AI/ML Engineer) ‚Äî Easy ‚Üí Hard\n",
    "\n",
    "**5) Hard ‚Äî Production NLP Failure Modes**\n",
    "An NLP model performs well in validation but fails in production.\n",
    "\n",
    "* List common NLP-specific failure modes.\n",
    "* How would you detect data drift in text?\n",
    "* How would you debug hallucinations or semantic errors in an LLM-based system?\n"
   ],
   "id": "5ff10b04959e9269"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**1) Easy ‚Äî Text Cleaning & Normalization**\n",
    "You‚Äôre given a column `review_text`.\n",
    "\n",
    "* What basic preprocessing steps would you apply before modeling?\n",
    "* Which steps depend on the downstream model (TF-IDF vs transformer)?\n",
    "\n",
    "**Answer:**\n",
    "1. Text Cleaning, that involves removing html tags, urls, PII, special characters, extra whitespaces, handling emojis and so on.\n",
    "2. Lower Casing; Converting all texts into lowercase to reduce vocab size.\n",
    "3. Tokenization - Splitting texts into small chunks called tokens.\n",
    "4. Stop words removal like is, the, in, and so on...\n",
    "5. Stemming - Removing suffixes from words - running, runs -> run. - reduces vocabulary size.\n",
    "6. Lemmatization - Reduces words to their base word from using linguistic rules.{better -> good}\n",
    "7. Named entity recognition - identifying real world entities\n",
    "8. spelling correction\n",
    "9. Feature extraction/Vectorization - Converting text into numerical form\n",
    "    - Bag of words\n",
    "    - TF-IDF\n",
    "    - Word Embeddings(Word2Vec, GloVe)\n",
    "    - Contextual Embeddings(BERT)\n",
    "\n",
    "II. TF-IDF - Needs high preprocessing steps, where as transformer needs lesser preprocessing.\n",
    "- Removal of stopwords - In TF-IDF: Stop words dominate frequency but add little value - So remove them, But in transformers we need to capture the meaning and attention patterns so include stopwords.\n",
    "- Stemming or Lemmatization: performing them in TF-IDF reduces sparcity {combine multiple similar words in single word}. Where as in Transformers model rely on grammar and tense - do not use them here.\n",
    "- Lower casing - TF-IDF: Always, Transformers: preserve case.\n",
    "- Tokenization: literal words are converted into token in TF-IDF, Transformer used advance methods to tokenize."
   ],
   "id": "b082856c72af28bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**2) Easy‚ÄìMedium ‚Äî Tokenization Strategies**\n",
    "Explain the difference between:\n",
    "\n",
    "* word-level tokenization\n",
    "* subword tokenization (BPE / WordPiece)\n",
    "* character-level tokenization\n",
    "\n",
    "When would each fail or succeed in real NLP systems?\n",
    "\n",
    "**Answer:**\n",
    "1. Word-Level tokenization\n",
    "- Split text into tokens based on words, usually spaces and punctuation.\n",
    "- Works well with NLP models, languages with clear word boundaries.\n",
    "- They are often small, clean vocabularies\n",
    "- `Fails` when misspellings, typos,on large vocab size.\n",
    "2. Subword Tokenization (BPE/Workpiece)\n",
    "- Breaks words into frequently occuring subword units learned from data.\n",
    "- \"unhappiness\" -> ['un', 'happi', 'ness'], \"ChatGPTlike\" -> ['Chat', 'GPT', 'like']\n",
    "- Works well with Modern Neural Networks, Handles unknown words, Controls vocab size.\n",
    "- Preserves meaning better than characters.\n",
    "- `Fails` on tokens that not allign linguistic meaning, Tokenization can be unintuitve.\n",
    "3. Characters-Level Tokenization\n",
    "- Treats every character as a token.\n",
    "- \"cat\" -> ['c', 'a', 't']\n",
    "- Extreme robustness to typos, noise.\n",
    "- Used in Special domains -> ['DNA', 'code', 'OCR text']\n",
    "- `Fails` on very long sequences, loses semantic structure, computationally expensive, harder to learn meaning."
   ],
   "id": "d84086a951c5bce2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**3) Medium ‚Äî Text Representation Choices**\n",
    "You need to build a classifier with **limited labeled data**.\n",
    "\n",
    "* Compare BoW, TF-IDF, word embeddings, and transformer embeddings.\n",
    "* Which would you choose and why?\n",
    "\n",
    "**Answer:**\n",
    "1. Bag of Words(BoW):\n",
    "- Represents text as word occurrnec counts\n",
    "- Very simple and fast | Works well with linear models | low risk of over fitting\n",
    "- Cons: No word importance, no semantics or context, very sparse high dimensional. Poor generalization with limited data.\n",
    "- Use when extremely small data, with baseline models.\n",
    "\n",
    "2. TF-IDF\n",
    "- Weighted version of BoW that downweights commons words and upweights informative ones.\n",
    "- Pros: Strong baseline for small datasets | Better signal than BoW | Works well with linear classifiers | Interpreable and stable.\n",
    "- Cons: Sparse, no semantics or word order, Cannot capture synonymy or context.\n",
    "- Use when limited label data\n",
    "\n",
    "3. Word Embeddings (Word2Vec/ GloVe/ FastText):\n",
    "- Dense vector representations learned from large corpora.\n",
    "- Pros: Capture semantic similarity, Dense lower dimensional, Can use pretrained embeddings\n",
    "- Cons: Context independent, Need pooling, Often underperform TF-IDF on small labelled datasets.\n",
    "- Use when moderal data size, semantic matter.\n",
    "\n",
    "4. Transformers Embeddings (BERT, RoBERTa)\n",
    "- Contextual embeddings pretrained on massive corpora.\n",
    "- Pros: Strong semantic understanding, handles context, Excellent at transfer learning\n",
    "- Cons: Computationally expensive, risk of overfitting with very small labeled data, more complex to train and tune\n",
    "- use when complex language understanding required."
   ],
   "id": "ab8f96e087668d70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**4) Medium‚ÄìHard ‚Äî Handling Long Documents**\n",
    "Your documents exceed the 512-token limit of BERT-like models.\n",
    "\n",
    "* What strategies would you use to handle this?\n",
    "* How do chunking and hierarchical models differ?\n",
    "* What are the trade-offs?\n",
    "\n",
    "Answer:\n",
    "1. Strategies to handle long documents\n",
    "- Split the documents in smaller chunks, process each chunk independently with BERT then aggregate.\n",
    "- Aggregation methods - mean, max pooling over chunk embeddings, attention based pooling, majortiy vote for classification.\n",
    "- Pros: easy to execute, words with standard BERT\n",
    "- Cons: Loses global document context, cross chunk dependencies are ignored, aggregation may dilute important signals.\n",
    "2. hierachial models - Model text at multiple levels (tokens -> sentences -> Docs)\n",
    "- Example: Sentence Encoder (BERT) -> Sentence embeddings\n",
    "- pros: preserves structure and long range dependencies | Better doc level undrestanding | Expensive."
   ],
   "id": "d9c9520e928aa2a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T05:38:07.357658Z",
     "start_time": "2026-01-28T05:38:07.219284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/NLP_covid/Corona_NLP_train.csv\", encoding = \"latin-1\")\n",
    "df.head()"
   ],
   "id": "f101456b07917aea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "90b11492a1929190"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Goal: 2 hour NLP Revision - 11:37",
   "id": "8ac6b247ea601429"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "After 2 hours, you should be able to:\n",
    "* Explain NLP systems as pipelines\n",
    "* Answer ‚Äúhow would you build‚Ä¶‚Äù questions\n",
    "* Discuss failure modes and tradeoffs\n",
    "* Avoid sounding academic or junior"
   ],
   "id": "37af742a94f94d19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> NLP is not about model but converting text messy text into reliable decisions.\n",
    "### NLP Pipeline\n",
    "Text ingestion -> Cleaning & Normalization -> Chunking/segmentation -> Task-Specific Model -> Post processing -> evaluation & monitoring.\n",
    "\n",
    "### 5 NLP problems\n",
    "> use case -> Model Choice -> Failure -> Fix\n",
    "1. Text Classification\n",
    "    - Use: Intent, routing, spam-non spam\n",
    "    - Failure: Class imbalance, ambiguity\n",
    "    - Fix: Thresholds, abstrain class\n",
    "        - In production not predicting is better than predicting wrong.\n",
    "\n",
    "2. Information Retrieval - Search engines, product search in e-comm, Legal document search.\n",
    "3. Text Generation - Generate human like text {examples: Text completion, chatbots, creative writing}\n",
    "4. Machine Translation - From one language to other\n",
    "5. Text Summarization - News summaries, Meeting notes generation, legal document summarie"
   ],
   "id": "6369dad1f08060d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## NLP Question set - NLTK & SpaCy - Day 4",
   "id": "1463b7d8a5103dda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### üü¢ Question 1 ‚Äî Easy (HR + Screening)\n",
    "\n",
    "**‚ÄúCan you explain the difference between NLTK and spaCy, and when you would use each?‚Äù**\n",
    "\n",
    "NLTK and SpaCY are Natural language processing libraries that allows us to work with Natural Language. Even though they both allows to do basic preprocessing like lemmatization, tokenization and so\n",
    "- NLTK is more of experiement centric, Used for classical ml pipelines such as POS, NER etc\n",
    "- SpaCy is advanced version developed for complex NLP problems. It provide high speed, scalable, and reliable application and sent for production.\n",
    "> \"NLTK is primarily used for linguistic exploration and classical NLP experimentation, while spaCy is designed for building fast, scalable, production-grade NLP pipelines with pretrained and custom models.\n",
    "\n",
    "**What I‚Äôm testing**\n",
    "\n",
    "* Conceptual clarity\n",
    "* Ability to communicate simply\n",
    "* Correct tool selection\n",
    "\n",
    "**Strong signal**\n",
    "\n",
    "* Mentions *NLTK for exploration* and *spaCy for production*\n",
    "* Avoids buzzwords\n",
    "\n",
    "**Red flag**\n",
    "\n",
    "* ‚ÄúThey are both NLP libraries; I use whichever is faster‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "#### üü° Question 2 ‚Äî Easy ‚Üí Medium (Applied NLP)\n",
    "\n",
    "**‚ÄúSuppose you are building a sentiment analysis system for customer reviews. Walk me through your NLP pipeline.‚Äù**\n",
    "\n",
    "You start with dataset curation/engineering -> data preprocessing which involves lemmatization/stemming, removal of stop words -> tokenization -> model training -> evaluation.\n",
    "\n",
    "> I‚Äôd start with data inspection to understand noise, imbalance, and language style, apply selective normalization, tokenize appropriately based on model choice, train a baseline before moving to more complex models, and evaluate using metrics like F1 while performing error analysis on misclassified reviews.\n",
    "\n",
    "**What I‚Äôm testing**\n",
    "\n",
    "* End-to-end thinking\n",
    "* Understanding of preprocessing\n",
    "* Awareness of real data issues\n",
    "\n",
    "**Strong signal**\n",
    "\n",
    "* Mentions normalization, tokenization, model choice, evaluation\n",
    "* Acknowledges noise, sarcasm, imbalance\n",
    "\n",
    "**Red flag**\n",
    "\n",
    "* Jumps straight to ‚Äúuse BERT‚Äù without pipeline context\n",
    "\n",
    "---\n",
    "\n",
    "#### üü† Question 3 ‚Äî Medium (System Design)\n",
    "\n",
    "**‚ÄúWe need a Named Entity Recognition system for legal documents. spaCy‚Äôs pretrained model performs poorly. What steps would you take?‚Äù**\n",
    "\n",
    "**What I‚Äôm testing**\n",
    "\n",
    "* Domain adaptation\n",
    "* Custom model training\n",
    "* Error analysis mindset\n",
    "\n",
    "**Strong signal**\n",
    "\n",
    "* Talks about annotation strategy\n",
    "* Mentions rule-based + ML hybrid\n",
    "* Mentions evaluation & iteration\n",
    "\n",
    "**Red flag**\n",
    "\n",
    "* ‚ÄúI‚Äôll try a bigger model‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "#### üîµ Question 4 ‚Äî Hard (Scaling & Collaboration)\n",
    "\n",
    "**‚ÄúWe suddenly need to process 50 million documents per month. What changes in your NLP system design?‚Äù**\n",
    "\n",
    "**What I‚Äôm testing**\n",
    "\n",
    "* Scalability thinking\n",
    "* Engineering maturity\n",
    "* Cross-functional awareness\n",
    "\n",
    "**Strong signal**\n",
    "\n",
    "* Mentions batching, streaming, memory, parallelism\n",
    "* Talks about infra, monitoring, tradeoffs\n",
    "* Knows spaCy strengths here\n",
    "\n",
    "**Red flag**\n",
    "\n",
    "* ‚ÄúJust add more servers‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "#### üî¥ Question 5 ‚Äî Very Hard (Senior / Founder-Level)\n",
    "\n",
    "**‚ÄúAn entity extraction model performs well in offline evaluation but fails badly in production. How do you debug this?‚Äù**\n",
    "\n",
    "**What I‚Äôm testing**\n",
    "\n",
    "* Real-world experience\n",
    "* Failure analysis\n",
    "* Ownership mindset\n",
    "\n",
    "**Strong signal**\n",
    "\n",
    "* Talks about data drift\n",
    "* Mentions logging, input distribution changes\n",
    "* Discusses annotation mismatch\n",
    "* Proposes monitoring & retraining loop\n",
    "\n",
    "**Red flag**\n",
    "\n",
    "* Blames the model without investigation\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What These Questions Reveal\n",
    "\n",
    "| Level | What I learn                               |\n",
    "| ----- | ------------------------------------------ |\n",
    "| Q1‚ÄìQ2 | Can you work in NLP at all?                |\n",
    "| Q3    | Are you an applied NLP engineer?           |\n",
    "| Q4    | Can you ship and scale?                    |\n",
    "| Q5    | Can I trust you with my startup‚Äôs core AI? |\n",
    "\n",
    "---\n",
    "\n",
    "## Coach‚Äôs advice (important)\n",
    "\n",
    "If you can **confidently answer Q3‚ÄìQ5**, you are:\n",
    "\n",
    "* Senior NLP Engineer\n",
    "* Strong startup hire\n",
    "* Someone I‚Äôd give ownership to\n",
    "\n",
    "If you want, next I can:\n",
    "\n",
    "* Provide **model answers**\n",
    "* Turn these into a **mock interview**\n",
    "* Help you prepare **whiteboard-style explanations**\n",
    "\n",
    "Just say the word.\n"
   ],
   "id": "55ead372fc1f528c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a19f2be90a3fb11d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# NLP - Text Classification project",
   "id": "9e138278843e8734"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## üìå Problem Statement: News Topic Classification Using NLP",
   "id": "2fb22b9d6ca827e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In today‚Äôs digital world, news platforms publish thousands of articles every day across multiple domains such as **business, sports, technology, and politics**. Manually categorizing these articles is time-consuming, error-prone, and not scalable.\n",
    "\n",
    "The objective of this project is to **automatically classify news articles into predefined topics** using Natural Language Processing (NLP) techniques.\n",
    "\n",
    "Specifically, the project aims to:\n",
    "\n",
    "1. Build a **baseline text classification model** using traditional NLP techniques such as **TF-IDF with Logistic Regression**.\n",
    "2. Build a **deep learning‚Äìbased classifier** using a **transformer model (DistilBERT)**.\n",
    "3. Compare the two approaches in terms of:\n",
    "\n",
    "   * accuracy\n",
    "   * precision, recall, and F1-score\n",
    "   * ability to capture semantic meaning\n",
    "4. Analyze model performance through **error analysis and confusion matrices**.\n",
    "5. Study the impact of **class imbalance** and understand **precision vs recall trade-offs**.\n",
    "\n",
    "The final outcome is to understand **when classical NLP methods are sufficient** and **when transformer-based models provide clear advantages**, considering both performance and computational cost.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Inputs and Outputs\n",
    "\n",
    "**Input:**\n",
    "Raw news article text.\n",
    "\n",
    "**Output:**\n",
    "One of the predefined categories:\n",
    "\n",
    "* Business\n",
    "* Sports\n",
    "* Technology\n",
    "* Politics\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Dataset\n",
    "\n",
    "The project uses the **AG News dataset**, a standard benchmark for text classification, containing labeled news articles across four categories.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Why this problem matters (interview angle)\n",
    "\n",
    "This problem represents a **real-world NLP use case** commonly found in:\n",
    "\n",
    "* news aggregation platforms\n",
    "* content recommendation systems\n",
    "* search engines\n",
    "* enterprise document classification\n",
    "\n",
    "It demonstrates core NLP skills including **text preprocessing, feature extraction, model selection, evaluation, and error analysis**.\n",
    "\n",
    "---\n",
    "\n",
    "### One-line version (if interviewer asks quickly)\n",
    "\n",
    "> ‚ÄúThe goal is to automatically classify news articles into categories using NLP, comparing traditional TF-IDF-based models with transformer-based models to understand performance and trade-offs.‚Äù\n"
   ],
   "id": "8d9722a12275e89d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üß≠ Roadmap: News Topic Classification (NLP Revision Project)\n",
    "\n",
    "#### üéØ Goal\n",
    "\n",
    "Classify news articles into:\n",
    "\n",
    "* business\n",
    "* sports\n",
    "* tech\n",
    "* politics\n",
    "\n",
    "Compare **traditional NLP** vs **transformer-based NLP**.\n",
    "\n",
    "---\n",
    "\n",
    "### üóìÔ∏è Phase 1 ‚Äî Data & Problem Setup (Day 1)\n",
    "\n",
    "##### 1Ô∏è‚É£ Dataset selection\n",
    "\n",
    "Use one of these:\n",
    "\n",
    "* **AG News**\n",
    "* **BBC News**\n",
    "* **20 Newsgroups (filtered)**\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "dataset[\"train\"].to_csv(\"../data/NLP_TextClassification/ag_news_train.csv\")\n",
    "dataset[\"test\"].to_csv(\"../data/NLP_TextClassification/ag_news_test.csv\")\n",
    "```\n",
    "\n",
    "Split:\n",
    "\n",
    "* 80% train\n",
    "* 10% validation\n",
    "* 10% test\n",
    "\n",
    "##### 2Ô∏è‚É£ Exploratory Data Analysis (EDA)\n",
    "\n",
    "Do this **before modeling**.\n",
    "\n",
    "‚úî Class distribution\n",
    "‚úî Average text length\n",
    "‚úî Vocabulary size\n",
    "\n",
    "> Interview point:\n",
    "> ‚ÄúI checked class imbalance and text length to choose model and max sequence length.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### üóìÔ∏è Phase 2 ‚Äî Baseline: TF-IDF + Logistic Regression (Day 1)\n",
    "\n",
    "##### 3Ô∏è‚É£ Text preprocessing\n",
    "\n",
    "Minimal, intentional preprocessing:\n",
    "\n",
    "* lowercase\n",
    "* remove punctuation\n",
    "* stopwords (optional)\n",
    "* no lemmatization (explain why)\n",
    "\n",
    "> Interview answer:\n",
    "> ‚ÄúHeavy preprocessing can remove useful signal for downstream models.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "##### 4Ô∏è‚É£ Feature extraction (TF-IDF)\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    max_features=50_000,\n",
    "    min_df=5\n",
    ")\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "```\n",
    "\n",
    "Explain:\n",
    "\n",
    "* Why ngrams?\n",
    "* Why max_features?\n",
    "* Why min_df?\n",
    "\n",
    "---\n",
    "\n",
    "##### 5Ô∏è‚É£ Train classifier\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Evaluate:\n",
    "\n",
    "* accuracy\n",
    "* precision\n",
    "* recall\n",
    "* F1 (macro)\n",
    "\n",
    "> Interview point:\n",
    "> ‚ÄúThis gives a fast, interpretable baseline.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### üóìÔ∏è Phase 3 ‚Äî Transformer Model (Day 2)\n",
    "\n",
    "##### 6Ô∏è‚É£ Tokenization\n",
    "\n",
    "Use **DistilBERT** first (lighter, faster).\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "```\n",
    "\n",
    "Explain:\n",
    "\n",
    "* Subword tokenization\n",
    "* Why max length (128 / 256)\n",
    "\n",
    "---\n",
    "\n",
    "##### 7Ô∏è‚É£ Fine-tuning model\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=4\n",
    ")\n",
    "```\n",
    "\n",
    "Train with:\n",
    "\n",
    "* small learning rate (2e-5)\n",
    "* 2‚Äì3 epochs\n",
    "* batch size 16\n",
    "\n",
    "> Interview point:\n",
    "> ‚ÄúTransformers capture contextual meaning, not just word frequency.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "##### 8Ô∏è‚É£ Evaluation & comparison\n",
    "\n",
    "Compare:\n",
    "\n",
    "| Model       | Accuracy | Macro F1 |\n",
    "| ----------- | -------- | -------- |\n",
    "| TF-IDF + LR | ~80%     | ~0.78    |\n",
    "| DistilBERT  | ~88‚Äì92%  | ~0.90    |\n",
    "\n",
    "---\n",
    "\n",
    "### üóìÔ∏è Phase 4 ‚Äî Error Analysis (Day 3)\n",
    "\n",
    "##### 9Ô∏è‚É£ Confusion matrix\n",
    "\n",
    "Identify:\n",
    "\n",
    "* sports vs business confusion\n",
    "* tech vs politics overlap\n",
    "\n",
    "> Interview gold:\n",
    "> ‚ÄúMost errors happen where semantic overlap exists.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "##### üîü Analyze misclassifications\n",
    "\n",
    "Check:\n",
    "\n",
    "* short texts\n",
    "* ambiguous headlines\n",
    "* domain-specific words\n",
    "\n",
    "---\n",
    "\n",
    "### üóìÔ∏è Phase 5 ‚Äî Class Imbalance & Metrics (Day 3)\n",
    "\n",
    "##### 1Ô∏è‚É£1Ô∏è‚É£ Handle class imbalance\n",
    "\n",
    "Options:\n",
    "\n",
    "* class weights\n",
    "* oversampling\n",
    "* focal loss (mention, don‚Äôt overdo)\n",
    "\n",
    "```python\n",
    "LogisticRegression(class_weight=\"balanced\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### 1Ô∏è‚É£2Ô∏è‚É£ Precision vs Recall discussion\n",
    "\n",
    "Explain with example:\n",
    "\n",
    "* **High precision** ‚Üí fewer false positives\n",
    "* **High recall** ‚Üí fewer false negatives\n",
    "\n",
    "> Interview answer:\n",
    "> ‚ÄúFor news categorization, balanced F1 matters more than raw accuracy.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Phase 6 ‚Äî Interview Explanation Cheat Sheet\n",
    "\n",
    "##### Why TF-IDF fails on semantic similarity?\n",
    "\n",
    "‚úî Word-based\n",
    "‚úî No context\n",
    "‚úî Synonyms treated differently\n",
    "\n",
    "Example:\n",
    "\n",
    "> ‚Äústock market‚Äù vs ‚Äúequity trading‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "##### Why transformers generalize better?\n",
    "\n",
    "‚úî Contextual embeddings\n",
    "‚úî Attention mechanism\n",
    "‚úî Pretrained on large corpora\n",
    "\n",
    "---\n",
    "\n",
    "##### How do you choose metrics?\n",
    "\n",
    "* Accuracy ‚Üí balanced data\n",
    "* Macro F1 ‚Üí class imbalance\n",
    "* Precision/Recall ‚Üí business needs\n",
    "\n",
    "---\n",
    "\n",
    "##### When would you NOT use transformers?\n",
    "\n",
    "‚úî Low latency systems\n",
    "‚úî Very small datasets\n",
    "‚úî Edge devices\n",
    "\n",
    "---\n",
    "\n",
    "### üìÅ Final Deliverables (VERY IMPORTANT)\n",
    "\n",
    "Your GitHub should include:\n",
    "\n",
    "* clean README\n",
    "* architecture diagram\n",
    "* comparison table\n",
    "* error analysis section\n",
    "* clear conclusions\n",
    "\n",
    "---\n",
    "\n",
    "### ‚è±Ô∏è Time estimate\n",
    "\n",
    "* **Fast learner**: 2 days\n",
    "* **Safe prep**: 3‚Äì4 days\n",
    "\n",
    "---\n",
    "\n",
    "If you want next, I can:\n",
    "\n",
    "* give **exact folder structure**\n",
    "* provide **starter code**\n",
    "* give **mock interview Q&A**\n",
    "* help you write a **perfect README**\n",
    "\n",
    "Just say what you want next.\n"
   ],
   "id": "604fef119449c7b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T09:21:55.128320600Z",
     "start_time": "2026-02-05T09:21:33.278452700Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7af3249349a31d23",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9185496cce514eaaa25bccc9ad36162f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5e73d5e73e7463594af254745c4a7c3"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4cacc0c448084ba6bfbc5a8cd8095b5c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad03858430464dafb4132eaad1053ee0"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b40bc2d7316a46dfa90df47eacf822b8"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 120000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 7600\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T09:27:40.821867300Z",
     "start_time": "2026-02-05T09:27:40.145606800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset[\"train\"].to_csv(\"../data/NLP_TextClassification/ag_news_train.csv\")\n",
    "dataset[\"test\"].to_csv(\"../data/NLP_TextClassification/ag_news_test.csv\")"
   ],
   "id": "710cd33b441ab76b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/120 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47c1f9de28b3491d90ea19ecddd4eff3"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "613bbf10610d4183a00d1647177df974"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "1830309"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T09:28:23.231368400Z",
     "start_time": "2026-02-05T09:28:22.765642600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/NLP_TextClassification/ag_news_train.csv')\n",
    "df.head()"
   ],
   "id": "fc73ded0bcd0957e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                text  label\n",
       "0  Wall St. Bears Claw Back Into the Black (Reute...      2\n",
       "1  Carlyle Looks Toward Commercial Aerospace (Reu...      2\n",
       "2  Oil and Economy Cloud Stocks' Outlook (Reuters...      2\n",
       "3  Iraq Halts Oil Exports from Main Southern Pipe...      2\n",
       "4  Oil prices soar to all-time record, posing new...      2"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T09:28:55.965942700Z",
     "start_time": "2026-02-05T09:28:55.931384600Z"
    }
   },
   "cell_type": "code",
   "source": "df['label'].value_counts()",
   "id": "9fcce577e98f720d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    30000\n",
       "3    30000\n",
       "1    30000\n",
       "0    30000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T09:29:26.671042600Z",
     "start_time": "2026-02-05T09:29:26.580751200Z"
    }
   },
   "cell_type": "code",
   "source": "dataset['train'][0]",
   "id": "28f33d7f2a10c395",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\",\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cb61ae00139f6818"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
