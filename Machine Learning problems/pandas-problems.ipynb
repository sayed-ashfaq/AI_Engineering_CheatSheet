{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-24T04:19:26.168434Z",
     "start_time": "2026-01-24T04:19:26.157970Z"
    }
   },
   "source": "print(\"Hi\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T01:35:04.608330Z",
     "start_time": "2026-02-04T01:34:59.722450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = {\n",
    "    \"customer_id\": [1,2,3,4,5,6,7,8,9,10],\n",
    "    \"age\": [25,34,45,29,41,37,23,52,31,27],\n",
    "    \"gender\": [\"M\",\"F\",\"M\",\"F\",\"F\",\"M\",\"F\",\"M\",\"M\",\"F\"],\n",
    "    \"city\": [\n",
    "        \"New York\",\"San Francisco\",\"Chicago\",\"New York\",\"Chicago\",\n",
    "        \"San Francisco\",\"New York\",\"Chicago\",\"San Francisco\",\"New York\"\n",
    "    ],\n",
    "    \"annual_income\": [50000,80000,60000,72000,65000,90000,48000,110000,75000,52000],\n",
    "    \"purchase_amount\": [120,250,90,300,None,450,60,500,200,150],\n",
    "    \"product_category\": [\n",
    "        \"Electronics\",\"Clothing\",\"Groceries\",\"Electronics\",\"Clothing\",\n",
    "        \"Furniture\",\"Groceries\",\"Electronics\",\"Clothing\",\"Electronics\"\n",
    "    ],\n",
    "    \"signup_date\": [\n",
    "        \"2022-01-10\",\"2021-11-03\",\"2020-06-15\",\"2022-03-22\",\"2021-07-19\",\n",
    "        \"2019-12-01\",\"2022-08-30\",\"2018-05-14\",\"2020-10-10\",\"2021-02-05\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()\n"
   ],
   "id": "432b29dfab62667b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   customer_id  age gender           city  annual_income  purchase_amount  \\\n",
       "0            1   25      M       New York          50000            120.0   \n",
       "1            2   34      F  San Francisco          80000            250.0   \n",
       "2            3   45      M        Chicago          60000             90.0   \n",
       "3            4   29      F       New York          72000            300.0   \n",
       "4            5   41      F        Chicago          65000              NaN   \n",
       "\n",
       "  product_category signup_date  \n",
       "0      Electronics  2022-01-10  \n",
       "1         Clothing  2021-11-03  \n",
       "2        Groceries  2020-06-15  \n",
       "3      Electronics  2022-03-22  \n",
       "4         Clothing  2021-07-19  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>city</th>\n",
       "      <th>annual_income</th>\n",
       "      <th>purchase_amount</th>\n",
       "      <th>product_category</th>\n",
       "      <th>signup_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>M</td>\n",
       "      <td>New York</td>\n",
       "      <td>50000</td>\n",
       "      <td>120.0</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>2022-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>F</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>80000</td>\n",
       "      <td>250.0</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>2021-11-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>M</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>60000</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Groceries</td>\n",
       "      <td>2020-06-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>F</td>\n",
       "      <td>New York</td>\n",
       "      <td>72000</td>\n",
       "      <td>300.0</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>2022-03-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>65000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>2021-07-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T01:35:07.936223Z",
     "start_time": "2026-02-04T01:35:07.921524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "X = [[100, 129, 157, 133], [168, 150, 30, 19], [4, 148, 106, 74], [123, 195, 60, 93], [169, 40, 188, 179], [40, 59, 29, 94], [165, 126, 16, 99], [167, 157, 65, 23], [128, 87, 37, 111], [191, 154, 89, 134], [101, 41, 145, 112], [43, 110, 197, 118], [147, 22, 109, 139], [11, 161, 135, 119], [26, 48, 199, 182], [96, 100, 82, 87], [149, 2, 8, 10], [5, 38, 166, 100], [193, 117, 59, 164], [133, 5, 38, 163], [88, 177, 84, 114], [9, 132, 177, 24], [94, 130, 83, 131], [77, 11, 141, 81], [154, 198, 175, 98], [21, 148, 170, 122], [185, 145, 101, 183], [100, 196, 111, 11], [97, 147, 112, 11], [25, 97, 95, 45], [6, 89, 88, 38], [51, 16, 151, 3], [90, 174, 122, 157], [2, 133, 121, 199], [15, 78, 163, 180], [103, 118, 7, 179], [102, 179, 157, 183], [113, 139, 195, 122], [55, 88, 68, 117], [115, 185, 93, 102], [139, 82, 3, 165], [135, 29, 78, 11], [11, 16, 60, 123], [103, 191, 187, 129], [146, 181, 28, 192], [85, 73, 136, 139], [117, 179, 81, 183], [15, 131, 106, 28], [58, 78, 111, 65], [76, 11, 25, 103], [11, 90, 162, 129], [144, 1, 16, 33], [33, 172, 40, 72], [106, 83, 160, 151], [68, 159, 150, 64], [31, 79, 83, 15], [51, 140, 173, 10], [105, 80, 70, 21], [195, 80, 64, 129], [50, 96, 107, 82], [185, 150, 15, 143], [28, 71, 27, 57], [58, 13, 146, 78], [20, 71, 183, 44], [91, 44, 15, 87], [77, 157, 95, 110], [132, 28, 193, 49], [177, 87, 57, 41], [194, 175, 17, 20], [166, 64, 134, 150], [79, 74, 162, 168], [166, 149, 34, 117], [160, 170, 127, 44], [99, 41, 103, 155], [48, 127, 138, 68], [17, 3, 101, 94], [29, 102, 123, 158], [194, 60, 135, 179], [73, 192, 145, 168], [21, 94, 154, 143], [17, 10, 145, 131], [73, 29, 195, 199], [132, 189, 90, 100], [134, 32, 81, 119], [118, 37, 119, 27], [51, 78, 187, 86], [95, 8, 56, 29], [156, 162, 186, 127], [126, 111, 144, 59], [7, 140, 32, 75], [40, 0, 109, 92], [165, 175, 61, 103], [178, 68, 185, 119], [132, 105, 36, 80], [165, 117, 35, 176], [128, 49, 185, 9], [50, 176, 12, 198], [124, 164, 99, 102], [36, 30, 114, 147], [166, 172, 35, 14]]\n",
    "y = [1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0]"
   ],
   "id": "25b425386cb7bd5b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T01:35:58.345282Z",
     "start_time": "2026-02-04T01:35:58.337442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = np.array(X)\n",
    "print(X.shape[0]) # rows\n",
    "print(X.shape[1])"
   ],
   "id": "8cc8b6e72d366935",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "4\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T01:52:57.630307Z",
     "start_time": "2026-02-04T01:52:57.617992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w = np.random.randn(4, 2)\n",
    "w"
   ],
   "id": "f7e6908d5c97f9ec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.43298931,  1.30977067],\n",
       "       [-0.0323946 ,  0.40205307],\n",
       "       [-0.11306775,  0.50350552],\n",
       "       [ 1.29550048,  0.74787416]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T02:06:13.040444Z",
     "start_time": "2026-02-04T02:06:13.031860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "b = np.zeros((1, 2))\n",
    "b"
   ],
   "id": "3e14eb9c944e08e1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T02:06:13.942614Z",
     "start_time": "2026-02-04T02:06:13.936320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "z = X @ w + b\n",
    "z.shape"
   ],
   "id": "6b235bf96c18c23a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T02:06:14.473161Z",
     "start_time": "2026-02-04T02:06:14.464964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = np.exp(z)\n",
    "a.shape"
   ],
   "id": "e85bf6cd04633bdb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T02:06:15.037392Z",
     "start_time": "2026-02-04T02:06:15.031888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = a / np.sum(a)\n",
    "a.shape"
   ],
   "id": "5e07d9ac011310b3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T02:00:58.649269Z",
     "start_time": "2026-02-04T02:00:58.639822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_hat = np.argmax(a, axis=1)\n",
    "y_hat"
   ],
   "id": "e7e8ac9f4fd08c1c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "315aa8798bb55276"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "75e4e84ffe489d22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "44c52090fafc0834"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c86dfb8044662166"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Day 1",
   "id": "edf41016732c215"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = {\n",
    "    \"user_id\": [\"U1\", \"U1\", \"U2\", \"U3\", \"U2\"],\n",
    "    \"order_id\": [\"O1\", \"O2\", \"O3\", \"O4\", \"O5\"],\n",
    "    \"order_time\": [\n",
    "        \"2024-01-01 10:00:00\",\n",
    "        \"2024-01-03 11:00:00\",\n",
    "        \"2024-01-01 09:30:00\",\n",
    "        \"2024-01-05 15:00:00\",\n",
    "        \"2024-01-07 18:00:00\",\n",
    "    ],\n",
    "    \"amount\": [500, 300, 700, 200, 1200],\n",
    "    \"category\": [\"electronics\", \"books\", \"electronics\", \"books\", \"furniture\"],\n",
    "}\n",
    "# pd.DataFrame(data)"
   ],
   "id": "bef0482b7a43b2db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tasks\n",
    "\n",
    "1. Identify repeat customers (‚â•2 orders)\n",
    "2. Compute average days between orders per repeat customer\n",
    "3. Find category-wise revenue contribution (%)\n",
    "4. Detect outlier transactions using IQR"
   ],
   "id": "de31f1cd094c0135"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "data = \"\"\"user_id,order_id,order_time,amount,category\n",
    "U1,O1,2024-01-01 10:00:00,500,electronics\n",
    "U1,O2,2024-01-03 11:00:00,300,books\n",
    "U2,O3,2024-01-01 09:30:00,700,electronics\n",
    "U3,O4,2024-01-05 15:00:00,200,books\n",
    "U2,O5,2024-01-07 18:00:00,1200,furniture\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv(StringIO(data), parse_dates=[\"order_time\"])\n",
    "df\n"
   ],
   "id": "a299e0a32de1bc84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# identify repeated customers greater than 2 orders\n",
    "\n",
    "order_counts = df.groupby(\"user_id\")['order_id'].count().reset_index()\n",
    "repeating_cus = order_counts[order_counts['order_id']>=2]['user_id']\n",
    "\n",
    "df_repeated = df[df['user_id'].isin(repeating_cus)]\n",
    "df_repeated.reset_index(drop=True, inplace=True)\n",
    "df_repeated"
   ],
   "id": "1f1934b85e3b6c00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Compute average days between orders per repeat customer\n",
    "\n",
    "# df_repeated.groupby('user_id')['order_time'].mean().dt.day ----- Incorrect solution\n",
    "\n",
    "df_repeated = df_repeated.sort_values(['user_id', 'order_time'])\n",
    "\n",
    "avg_days = (\n",
    "    df_repeated\n",
    "    .groupby('user_id')['order_time']\n",
    "    .diff()\n",
    "    .dt.days\n",
    "    .groupby(df_repeated['user_id'])\n",
    "    .mean()\n",
    ")\n",
    "avg_days"
   ],
   "id": "c5b63168ac51aa67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_repeated.groupby('user_id')['order_time'].mean().dt.day #----- Incorrect solution\n",
   "id": "82eebdb7ade8198d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Find category-wise revenue contribution (%)\n",
    "import numpy as np\n",
    "\n",
    "df.groupby('category')['amount'].apply(lambda x: np.sum(x)*100/np.sum(df['amount']))"
   ],
   "id": "38262d0a938aea9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Detect outlier transactions using IQR\n",
    "\n",
    "# IQR = Q3 - Q1\n",
    "\n",
    "# upper boundary for outliers = Q3 + 1.5*IQR\n",
    "# lower boundary = Q1 - 1.5*IQR\n",
    "\n",
    "q1 = df['amount'].quantile(0.25)\n",
    "q3 = df['amount'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "print(q1, q3, iqr)"
   ],
   "id": "cd11aed5a105c8d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "up = q3 + 1.5*iqr\n",
    "dn = q1 - 1.5*iqr\n",
    "df[(df['amount'] > up) | (df['amount'] < dn)]"
   ],
   "id": "c5a126a3e055a71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(up, dn)",
   "id": "73ab001ec79e3e0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Day 2",
   "id": "f86df5e0f77ad95c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. **Easy:** What is a pandas `DataFrame`, and how is it different from a pandas `Series`?\n",
    "\n",
    "2. **Easy‚ÄìMedium:** How do you load a CSV file into pandas and display the first 10 rows?\n",
    "\n",
    "3. **Medium:** What is the difference between `.loc[]` and `.iloc[]`, and when would you use each?\n",
    "\n",
    "4. **Medium‚ÄìHard:** Given a DataFrame with missing values, how would you (a) detect them, and (b) handle them differently depending on whether the column is numeric or categorical?\n",
    "\n",
    "5. **Hard:** You have a 50-million-row DataFrame that does not fit comfortably in memory. What pandas-based strategies (or pandas-adjacent tools) would you use to process it efficiently, and why?\n"
   ],
   "id": "2580cf612ca399f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Pandas Dataframe is the collection of multidimensional arrays labelled as rows and columns whereas Series is the Single dimensional array\n",
    "2. ```python\n",
    "    df = pd.read_csv(\"file.csv\")\n",
    "    df.head(10)\n",
    "   ```\n",
    "3. .loc[] is label based indexing, where you can specify the label name or row no to select.\n",
    "    - .iloc[] is 0 based indexing, which only take integer index values.\n",
    "    - loc is used when selecting data based on condition.\n",
    "    - iloc is used when you want to select based on index, or performing slicing in both dimensions."
   ],
   "id": "9970da38ae90671"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"file.csv\")\n",
    "df.head(10)"
   ],
   "id": "5875e0b3c42e7179",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.isna().sum() # detect them\n",
    "df['amount'].fillna(np.mean(df['amount']), inplace=True) # numberical, remove, imputation\n",
    "df.fillna(df.mode().iloc[0], inplace=True)"
   ],
   "id": "46853f750c9a85db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## 5 ##\n",
    "\n",
    "# pandas based strategies\n",
    "def process(chunk):\n",
    "    return chunk[chunk['amount']>0]\n",
    "\n",
    "for chunk in pd.read_csv(\"file.csv\", chunksize=1000):\n",
    "    process(chunk)\n",
    "\n",
    "## Reads data in managable pieces\n",
    "## used mainly for filtering and aggregation\n",
    "\n",
    "# optimize dtypes\n",
    "df['cat_col'] = df['cat_col'].astype(\"category\")\n",
    "df['num_col'] = df['num_col'].astype(\"int32\") # Reduces memory footprint dramatically.\n",
    "\n",
    "\n",
    "# usecols method and filtering early\n",
    "pd.read_csv('big.csv', usecols=['id', 'value'])\n",
    "## avoid loading unnecesary data\n",
    "\n",
    "###################################\n",
    "'''\n",
    "# Pandas-adjacent tools:\n",
    "\n",
    "4. Dask\n",
    "- Parallel, out-of-core DataFrame API similar to pandas.\n",
    "- Scales to datasets larger than RAM.\n",
    "\n",
    "5. PyArrow / Parquet\n",
    "- Columnar formats ‚Üí faster reads + lower memory usage.\n",
    "- Works well with pandas, Dask, Spark.\n",
    "\n",
    "6. Database-backed workflows\n",
    "- Use SQL (Postgres, DuckDB, SQLite) for filtering/aggregation.\n",
    "- Load only final results into pandas.\n",
    "'''\n"
   ],
   "id": "560b870eb8b089f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Day 1",
   "id": "3e6c4767b8e0de41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You load a CSV into pandas and notice one column that should be numeric is of type object.<br>\n",
    "How would you safely convert it to a numeric dtype, and what edge cases would you watch out for in production data?"
   ],
   "id": "5853d6496b549b5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['col'] = df['col'].astype('float64')\n",
    "\n",
    "# edge cases would be data inconsistency there might be null values which causes error in coversion, so impute the values with some constant like 0 then convert it into dtype."
   ],
   "id": "4ac6fc19a69fe54a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In pandas, arithmetic operations automatically align on index labels.<br>\n",
    "Describe a real-world scenario where this behavior is helpful, and another where it could silently introduce a bug. How would you detect and prevent that bug?\n",
    "\n",
    "**Answer**:<br>\n",
    "When you have two columns say revenue and cost, with months as index, cost is missing a index.\n",
    "- When you perform arithmetic ops such as profit = revenue - cost\n",
    "- Only matched indexes get subtracted.\n",
    "\n",
    "The silent bug here is,\n",
    "- when the index is missing in col that is in another col\n",
    "- after performing calculation the resultant index converts into null value. Which is a information loss.\n",
    "\n",
    "|month | revenue | cost |\n",
    "|---|---|---|\n",
    "|jan   |  1000   | 200  |\n",
    "|feb   |  1100   | 250  |\n",
    "|Mar   |  1200   |      |\n",
    "|Apr   |  1300   | 300  |\n",
    "\n",
    "profit = revenue - cost\n",
    "\n",
    "month | profit\n",
    "|---|---|\n",
    "jan | 800\n",
    "feb | 850\n",
    "mar | NaN\n",
    "Apr | 1000"
   ],
   "id": "caedd385edacdeb7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Day 5\n",
    "Pandas for AI/ML Engineer Interviews ‚Äî Easy ‚Üí Hard (Set 2)"
   ],
   "id": "c23ffd22fef54de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**1) Easy ‚Äî Feature Selection & Cleanup**\n",
    "You‚Äôre given a training DataFrame with 100 columns.\n",
    "\n",
    "* Identify non-numeric columns.\n",
    "* Drop ID-like columns that should not be used for training.\n",
    "* Separate features `X` and target `y`."
   ],
   "id": "186a68c6a4f70df4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ffca154a491f3bbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# one way to identify non numeric columns\n",
    "# df.info()\n",
    "\n",
    "# Since there are 100 columns, we get the only those columns which are non numeric\n",
    "non_numeric_cols = df.select_dtypes(exclude=['number']).columns.to_list()\n",
    "print(\"Non numeric columns:\", non_numeric_cols)\n",
    "df.select_dtypes(exclude=['number']).head()"
   ],
   "id": "4b70c587b7db618",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Drop columns like ids that are not used for Training\n",
    "df.drop(labels = 'customer_id', inplace= True, axis= 1)"
   ],
   "id": "dabed2deae232ae0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# separate features x and y\n",
    "\n",
    "## First convert categorical variable into numeric using one hot encoding\n",
    "df_encoded = pd.get_dummies(df, columns=['gender', 'city', 'product_category'])\n",
    "\n",
    "X = df_encoded.drop('purchase_amount', axis=1)\n",
    "y = df_encoded['purchase_amount']"
   ],
   "id": "7fd934a30ca25151",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**2) Easy‚ÄìMedium ‚Äî Handling Missing Values (Train vs Test)**\n",
    "You have `train_df` and `test_df`.\n",
    "\n",
    "* Fill missing numerical values using statistics from **only** the training set.\n",
    "* Apply the same transformation to the test set.\n",
    "* Explain why doing this jointly causes data leakage."
   ],
   "id": "d1334fd6f7c69851"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T05:50:45.958965Z",
     "start_time": "2026-01-21T05:50:34.082610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "62608a1dee89c425",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T05:53:10.999605Z",
     "start_time": "2026-01-21T05:53:10.985980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#  Fill missing numerical values using statistics from **only** the training set.\n",
    "# Apply the same transformation to the test set.\n",
    "# Explain why doing this jointly causes data leakage.\n",
    "\n",
    "num_cols = df.select_dtypes(include=['number']).columns.to_list()\n",
    "for col in num_cols:\n",
    "    X_train.fillna(np.mean(X_train[col]), inplace=True)\n",
    "    X_test.fillna(np.mean(X_test[col]), inplace=True)\n",
    "\n",
    "# When you do this jointly the mean is shared across test and train data set, which cause data leakage.\n"
   ],
   "id": "fbfe1191227afe78",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**3) Medium ‚Äî Encoding Categorical Variables**\n",
    "A dataset has columns: `city` (high cardinality), `gender` (low cardinality).\n",
    "\n",
    "* Which encoding techniques would you choose for each and why?\n",
    "* How would you handle unseen categories at inference time?"
   ],
   "id": "c6c0cfbd3d08f281"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# If the city has high cardinality then I would choose one-hot encoding, label encoding gives the rank which make the model think one is superior. So i have to give equal importance to each category.\n",
    "#     - Where as gender i would use binary encoding/label encoding(0, 1).\n",
    "\n",
    "# The unseen categories can because of spelling error, so i would make sure using pipeline where the categories names converted into lower casing, remove spaces in between them"
   ],
   "id": "287bcc2cd64995f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**4) Medium‚ÄìHard ‚Äî Data Drift & Validation**\n",
    "You receive daily inference data in production.\n",
    "\n",
    "* Using pandas, how would you detect **feature distribution drift** between training data and live data?\n",
    "* Which statistical metrics would you compute?\n",
    "* How would this integrate into an automated pipeline?"
   ],
   "id": "c722902e7ce6e057"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. I would detect the feature distribution by performing statistical test such as PsI(pop statbility index) or KS test. if p < 0.05 it means there is drift detected.\n",
    "2. To identify the drift i would monitor the accuracy of model, false positives and more importantly roc-auc curve.\n",
    "3. I would include this in pipeline via triggerd mechanism, if the error/accuracy/roc-auc curve falls below certain threshold then it will trigger alert and then the model retrains on current data or human can take care analyze and do some changes in the model."
   ],
   "id": "fea6a381ee0d2c5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**5) Hard ‚Äî Label Leakage & Pipeline Design**\n",
    "You‚Äôre building a churn prediction model.\n",
    "\n",
    "* Identify potential sources of label leakage in tabular data.\n",
    "* Show how you‚Äôd design a pandas + sklearn pipeline to prevent leakage.\n",
    "* Explain how this pipeline behaves during training vs inference.\n",
    "\n",
    "---\n",
    "Here‚Äôs a **concise, interview-ready answer** üëá\n",
    "\n",
    "---\n",
    "\n",
    "### **Potential Sources of Label Leakage**\n",
    "\n",
    "* Features generated **after the prediction time** (e.g., ‚Äúdays since last cancellation‚Äù)\n",
    "* Aggregates using **full dataset** (mean, target encoding)\n",
    "* Target-derived features (e.g., ‚Äúchurn_count_last_30_days‚Äù)\n",
    "* Using **test data statistics** during preprocessing\n",
    "* Leakage through time (future transactions, payments, support tickets)\n",
    "\n",
    "---\n",
    "\n",
    "### **Leakage-Safe Pandas + Sklearn Pipeline**\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols),\n",
    "    (\"cat\", cat_pipe, cat_cols)\n",
    "])\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"clf\", LogisticRegression())\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Training vs Inference Behavior**\n",
    "\n",
    "* **Training:** pipeline `fit()` learns imputation values, encodings, scalers **only from training data**\n",
    "* **Inference:** pipeline `transform()` applies the **same learned transformations** without re-fitting\n",
    "* Prevents leakage by **locking preprocessing to train-time statistics**\n"
   ],
   "id": "b207a8e4416f2087"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Day 6\n",
    "### Pandas for AI/ML Engineer Interviews ‚Äî Easy ‚Üí Hard (Set 3)\n"
   ],
   "id": "ed521a2501246a6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Pandas for AI/ML Engineer Interviews ‚Äî Easy ‚Üí Hard (Set 3)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**3) Medium ‚Äî Feature Correlation & Multicollinearity**\n",
    "Given 50 numerical features:\n",
    "\n",
    "* Compute correlation matrix efficiently.\n",
    "* Identify highly correlated feature pairs (> 0.9).\n",
    "* Explain how multicollinearity affects linear models vs tree-based models.\n",
    "\n",
    "---\n",
    "\n",
    "**4) Medium‚ÄìHard ‚Äî Time-Based Train/Validation Split**\n",
    "You‚Äôre working with time-series data.\n",
    "\n",
    "* Split data into train/validation sets without data leakage.\n",
    "* Show how you‚Äôd verify the split is correct using pandas.\n",
    "* Explain why random split is dangerous here.\n",
    "\n",
    "---\n",
    "\n",
    "**5) Hard ‚Äî End-to-End Offline ‚Üí Online Consistency**\n",
    "You trained a model using pandas-based feature engineering.\n",
    "\n",
    "* How do you guarantee the **exact same transformations** are applied in production?\n",
    "* What breaks if pandas logic is reimplemented separately for training and inference?\n",
    "* Describe a robust production-safe design.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, next we can:\n",
    "\n",
    "* Convert these into **mock interview questions** with ideal answers, or\n",
    "* Drill into **one question deeply** with code + explanation, or\n",
    "* Switch from pandas to **system design questions for AI engineers**.\n"
   ],
   "id": "5291939f06cef0dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**1) Easy ‚Äî Dataset Sanity Checks**\n",
    "You receive a raw dataset before model training.\n",
    "\n",
    "* Check for duplicate rows.\n",
    "* Verify there are no negative values in columns that should be positive (e.g., `age`, `amount`).\n",
    "* Ensure the target variable has no missing values.\n",
    "\n",
    "**Answer**:<br>\n",
    "- `df.duplicated()` - Returns boolean series // `df.duplicated().sum()` - Gives total count of duplicate values.\n",
    "- To verify there are no negative vales you can filter them out using masking : `df[(df['age'] >  0) & (df['amount'] > 0)]`.\n",
    "    - or using any or all functions; `(df['age'] < 0).any()` - Gives boolean True or False\n",
    "- `df = df.dropna(subset=['target_variable'])`"
   ],
   "id": "f7cb262af4fbf50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**2) Easy‚ÄìMedium ‚Äî Class Imbalance Analysis**\n",
    "You‚Äôre training a binary classification model.\n",
    "\n",
    "* Using pandas, analyze class distribution.\n",
    "* Compute class imbalance ratio.\n",
    "* Explain when imbalance becomes a modeling problem and when it doesn‚Äôt.\n",
    "\n",
    "**Answer:**\n",
    "- Using value counts `s = df['col'].value_counts()`\n",
    "- `df['col'].value_counts()/len(df)` OR `imb_ratio = s.min()/s.max()`\n",
    "- When there is High imbalanced data the model metrics will most likely misleading.\n",
    "- The accuracy might be higher but it fails to predict the class with least data.\n",
    "\n",
    "Correct Answer\n",
    "- Class imbalance becomes a problem when the minority class is important and misclassification cost is high; otherwise, models can still perform well with proper metrics.\n",
    "- In classification we use precision/recall, F1, ROC-AUC, PR-AUC metrics to rank/Evaluate the model."
   ],
   "id": "2728a9953ebaa103"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T07:31:52.886650Z",
     "start_time": "2026-01-22T07:31:48.017405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "  \"name\": [\"John\", \"Mary\", \"John\", \"Sally\", \"Mary\"],\n",
    "  \"age\": [40, 30, 40, 50, 30],\n",
    "  \"city\": [\"Bergen\", \"Oslo\", \"Stavanger\", \"Oslo\", \"Oslo\"],\n",
    "  'class': [1, 0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ],
   "id": "ce75bbc850cc25c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "1    3\n",
      "0    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T07:32:20.228021Z",
     "start_time": "2026-01-22T07:32:20.218076Z"
    }
   },
   "cell_type": "code",
   "source": "df['class'].value_counts()/len(df)",
   "id": "949eff0e0a7fb509",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "1    0.6\n",
       "0    0.4\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T07:34:12.588145Z",
     "start_time": "2026-01-22T07:34:12.581314Z"
    }
   },
   "cell_type": "code",
   "source": "s.min()/s.max()",
   "id": "41e1bb24436535ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Day 7-8\n",
   "id": "19e15cf2d6b64080"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**1) Easy ‚Äî Column-Level Statistics for EDA**\n",
    "Given a DataFrame with mixed data types:\n",
    "\n",
    "* Identify numerical vs categorical columns.\n",
    "* Compute mean, median, std for numerical columns.\n",
    "* Compute value counts and top-k categories for categorical columns.\n",
    "\n",
    "**Answer:**<br>\n",
    "```python\n",
    "df.info() # -- To identify the numerical and categorical columns\n",
    "df.describe() # -- To compute mean, median and std for all numerical columns\n",
    "\n",
    "def cat_value_counts(col, k): # -- This will give the value counts of catgorical columns and top k categories as well.\n",
    "    return df[col].value_counts()[:k]\n",
    "\n",
    "cat_value_counts('gender', 1)\n",
    "```"
   ],
   "id": "d7c5a04ba38a517f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**2) Easy‚ÄìMedium ‚Äî Outlier Detection**\n",
    "You have a numerical feature `transaction_amount`.\n",
    "\n",
    "* Detect outliers using IQR in pandas.\n",
    "* Remove or cap extreme values.\n",
    "* Explain when removing outliers is a bad idea in ML.\n",
    "\n",
    "**Answer:**<br>\n",
    "```python\n",
    "q3 = df['transaction_amount'].quantile(0.75)\n",
    "q1 = df['transaction_amount'].quantile(0.25)\n",
    "iqr = q3 - q1\n",
    "upper_boundary = q3 + 1.5*iqr\n",
    "lower_boundary = q1 - 1.5*iqr\n",
    "\n",
    "# get the outliers\n",
    "print(df[(df['transaction_amount'] > upper_boundary) & (df['transaction_amount'] < lower_boundary)])\n",
    "\n",
    "# remove extreme values - I am assuming outliers\n",
    "df = df[(df['transaction_amount'] < upper_boundary) & (df['transaction_amount'] > lower_boundary)]\n",
    "```\n",
    "- Sometimes outliers carry important information such as a customer dealing with millions where as most of the data is capped to 100. The customer could be valueable asset or could be anomoly. So if we remove the outlier we might lose that information. So in order to deal with outliers you should have a good understaning of problem statement."
   ],
   "id": "f24879d90e99edcb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**3) Medium ‚Äî Target Encoding Safely**\n",
    "You want to use **target encoding** for a categorical feature.\n",
    "\n",
    "* How would you compute it using pandas?\n",
    "* How do you avoid target leakage?\n",
    "* How would this work with cross-validation?\n",
    "\n",
    "**Answer**:<br>\n",
    "- Target encoding means: Replace each word with the average value of what you're trying to predict.\n",
    "- It's usually mean but more precisely target encoding is replacing the categories with value that has effect on the target\n",
    "- on binary encoding you can use probability $p(t=1/x=ci)$\n",
    "- t is the target (0 , 1), ci is the i'th category.\n",
    "-\n",
    "```python\n",
    "# 1. Encoding categorial variables using pandas\n",
    "stats = df['target'].groupby(df['product_category']).agg(['count', 'mean'])\n",
    "df['product_category_encoded_count']= df['product_category'].map(stats['mean'])\n",
    "df.head()\n",
    "```\n",
    "\n",
    "-  In order to avoid target leakage you should split the data into train and test, then perform encoding on train dataset.\n",
    "i haven't done here but that's how you do it.\n",
    "\n",
    "- Cross-validation splits data into multiple folds so that each sample is evaluated on unseen data.\n",
    "For target encoding, we compute category statistics using only the training folds and apply them to the validation fold, preventing target leakage."
   ],
   "id": "62777dc136e06b9a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**4) Medium‚ÄìHard ‚Äî Feature Stability Across Time**\n",
    "You have monthly snapshots of data.\n",
    "\n",
    "* Using pandas, check whether feature distributions are stable over time.\n",
    "* Which metrics would you track?\n",
    "* How would unstable features affect model performance?\n",
    "\n",
    "**Answer:**<br>\n"
   ],
   "id": "90b5012a5324fe3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**5) Hard ‚Äî Debugging a Failing Model Using Pandas**\n",
    "A model performed well offline but fails badly in production.\n",
    "\n",
    "* What pandas analyses would you run first?\n",
    "* How would you compare training vs inference datasets?\n",
    "* Which red flags immediately indicate data or feature issues?\n",
    "\n",
    "**Answer:**<br>"
   ],
   "id": "56f91da632f640c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T16:42:11.392754Z",
     "start_time": "2026-02-02T16:42:11.382429Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Hey\")",
   "id": "6b953abe97b38a37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Day 17 - Random questions in Machine Learning",
   "id": "3545deccfaf75d45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### AI/ML Engineer Interview Questions ‚Äî Random Mix (Easy ‚Üí Hard)\n",
    "\n",
    "**1) Easy ‚Äî Probability & Intuition**\n",
    "You flip a biased coin where\n",
    "$( P(\\text{Head}) = 0.7 )$.\n",
    "\n",
    "* What is the probability of getting **at least one head** in 3 flips?\n",
    "* How would you simulate this in code to verify?\n",
    "\n",
    "- P(Tail) would be 1 - p(head) which is 0.3\n",
    "- p(zero heads) = 0.3 **3\n",
    "- p(atleast 1 head) = 1 - 0.027 = 0.973\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**2) Easy‚ÄìMedium ‚Äî Data Leakage (Trick Question)**\n",
    "Your validation accuracy is **higher than training accuracy**.\n",
    "\n",
    "* List valid reasons this can happen.\n",
    "* List reasons that indicate a serious bug or leakage.\n",
    "\n",
    "---\n",
    "\n",
    "**3) Medium ‚Äî System Design (ML-flavored)**\n",
    "Design a system to **rank search results** for an e-commerce website.\n",
    "\n",
    "* What data do you collect?\n",
    "* Which models would you start with?\n",
    "* How would you evaluate offline vs online?\n",
    "\n",
    "---\n",
    "\n",
    "**4) Medium‚ÄìHard ‚Äî Optimization & Generalization**\n",
    "Your training loss keeps decreasing, but validation loss increases.\n",
    "\n",
    "* What exactly is happening mathematically?\n",
    "* Why doesn‚Äôt ‚Äúmore training‚Äù fix this?\n",
    "* Name at least 4 concrete countermeasures.\n",
    "\n",
    "---\n",
    "\n",
    "**5) Hard ‚Äî Real-World Failure Scenario**\n",
    "You deploy an ML model that directly affects revenue.\n",
    "One week later, revenue drops by 20%.\n",
    "\n",
    "* How do you **prove** the model is responsible?\n",
    "* What data would you pull first?\n",
    "* What is your rollback and mitigation strategy?\n",
    "\n",
    "---\n",
    "\n",
    "If you want, next I can:\n",
    "\n",
    "* Go **pure math** (probability, linear algebra, optimization),\n",
    "* Go **hardcore systems** (ML infra, latency, cost), or\n",
    "* Run a **stress-test mock interview** where I interrupt and challenge your answers.\n"
   ],
   "id": "4b2d25144d354477"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "96c37abe0cb8e061"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
