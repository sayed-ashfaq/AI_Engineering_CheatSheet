{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-03T07:11:24.576423Z",
     "start_time": "2026-02-03T07:11:24.568253Z"
    }
   },
   "source": "print(\"Hey\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Deep Learning",
   "id": "7db3bf7576b91acc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Day 2 - Deep Learning Interview Questions (AI/ML Engineer) — Easy → Hard",
   "id": "a128ceed59f1c996"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**1) Easy — Neural Network Fundamentals**\n",
    "\n",
    "* What are model parameters vs hyperparameters?\n",
    "* Why do we need non-linear activation functions?\n",
    "* What happens if you stack only linear layers?\n",
    "\n",
    "> Answer\n",
    "1. para vs Hyper_para\n",
    "* Model parameters are the weights of neurons and biases in a model, which are learned while training during back propagation.\n",
    "* Hyperparameters are the values set by humans to control the training dynamics and model capacity such as learning rate, batchsize, number of layers.\n",
    "2. Non linear activation functions (ReLU, Sigmoid, Tanh, Softmax)\n",
    "* These non linear functions helps the model to understand complex patterns. This is what separate Deep learning models with simple machine learning models like linear regression.\n",
    "3. When you stack only linear layers, multiple linear layers collapse into a single linear transformation. and the model wouldn't able to capture the non linear patterns in the data.\n",
    "---\n",
    "\n",
    "**2) Easy–Medium — Loss Functions & Problem Mapping**\n",
    "You’re given different tasks.\n",
    "\n",
    "* Binary classification\n",
    "* Multi-class classification\n",
    "* Regression with outliers\n",
    "\n",
    "Which loss function would you choose for each, and why?\n",
    "\n",
    "> Answer:\n",
    "\n",
    "1. Binary classification - Binary Cross entropy/Logloss - classification is done by using probability, since there are only two possible values. Thus to determine the loss between the actual and predicted values i would choose the binary CE.\n",
    "2. Multiclass classification - categorical cross entropy - Where there are more than 2 classes binary entropy can't calculate the loss, so categorical cross entropy is the best choice.\n",
    "3. Regression - MSE, MAE. MSE finds the average squared distance from target to the predicted value which makes it suitable for regression.\n",
    "    - If there are outliers in the dataset then MSE is very bad with outliers, so MAE that calculate average distance would be suitable.\n",
    "---\n",
    "\n",
    "**3) Medium — Optimization Dynamics**\n",
    "\n",
    "* Compare SGD, Momentum, RMSProp, and Adam.\n",
    "* Why does Adam converge faster but sometimes generalize worse?\n",
    "* When would you explicitly prefer plain SGD?\n",
    "\n",
    "---\n",
    "\n",
    "**4) Medium–Hard — Generalization & Regularization**\n",
    "A deep model overfits badly.\n",
    "\n",
    "* How do dropout, weight decay, and data augmentation differ in effect?\n",
    "* Why can dropout hurt performance in some architectures?\n",
    "* How do batch size and learning rate interact with generalization?\n",
    "\n",
    "---\n",
    "\n",
    "**5) Hard — Training Instability & Debugging**\n",
    "Your deep model:\n",
    "\n",
    "* trains fine for a few epochs, then loss explodes\n",
    "* gradients become zero in early layers\n",
    "\n",
    "How would you **systematically debug** this?\n",
    "Explain at **data, architecture, initialization, and optimizer** levels.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, next I can:\n",
    "\n",
    "* Convert these into **whiteboard-style answers**,\n",
    "* Ask **follow-up drill questions** interviewers actually ask, or\n",
    "* Jump to **LLM / transformer-specific deep learning questions**.\n"
   ],
   "id": "8d4b97bcadcd9a11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "23ac396e64827649"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
