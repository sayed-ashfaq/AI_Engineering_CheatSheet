{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# Deep Learning Fundamentals Revision",
   "id": "3cb379b273663733"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Topics covering\n",
    "* [X] ~~Neural network Fundamentals - Linearity/Non linearity~~\n",
    "* [x] ~~Activation Functions~~\n",
    "* [ ] Loss Functions - Cross Entropy, MSE vs MAE, Perplexity\n",
    "* [ ] Optimizers\n",
    "* [ ] Learning Rate Strategies\n",
    "* [ ] Regularization\n",
    "* [ ] Normalization\n",
    "* [ ] Transformers arch and Attention\n",
    "* [ ] Evaluation\n",
    "\n",
    "Every topic you review must answer 3 questions:\n",
    "\n",
    "* What problem does this solve?\n",
    "* When does it fail?\n",
    "* What knob do I turn?\n",
    "\n",
    "If you canâ€™t answer those â†’ you donâ€™t â€œknowâ€ it yet."
   ],
   "id": "abf29e40b839eb57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### How to Study like an Engineer not a student",
   "id": "ec0395038809e482"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Good â€” this is the *right* question.\n",
    "Hereâ€™s the **concrete, no-nonsense answer** to *â€œwhat am I supposed to do?â€* given that expectation.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸŽ¯ What You Are ACTUALLY Supposed to Do (One-Day Mode)\n",
    "\n",
    "Your job is **not** to study more theory.\n",
    "Your job is to practice **thinking like the engineer who owns the model**.\n",
    "\n",
    "Below is a **step-by-step action plan** you can literally follow today.\n",
    "\n",
    "---\n",
    "\n",
    "##### 1ï¸âƒ£ Switch Your Study Mode (Non-Negotiable)\n",
    "\n",
    "Stop asking:\n",
    "\n",
    "> â€œDo I remember this?â€\n",
    "\n",
    "Start asking:\n",
    "\n",
    "> â€œIf this breaks in production, what would I change first?â€\n",
    "\n",
    "Every topic you review must answer **3 questions**:\n",
    "\n",
    "* What problem does this solve?\n",
    "* When does it fail?\n",
    "* What knob do I turn?\n",
    "\n",
    "If you canâ€™t answer those â†’ you donâ€™t â€œknowâ€ it yet.\n",
    "\n",
    "---\n",
    "\n",
    "##### 2ï¸âƒ£ Do This Loop for Every Topic (THE Core Skill)\n",
    "\n",
    "###### The **Engineer Loop** (repeat 8â€“10 times today)\n",
    "\n",
    "For **each** concept (e.g. AdamW, LoRA, attention):\n",
    "\n",
    "1. **Explain it out loud in plain English** (no math)\n",
    "2. **Name 1 reason it fails**\n",
    "3. **Name 1 trade-off**\n",
    "\n",
    "Example (AdamW):\n",
    "\n",
    "* What it is: Adaptive optimizer with decoupled weight decay\n",
    "* Fails when: LR too high â†’ unstable fine-tuning\n",
    "* Trade-off: Faster convergence vs less predictable generalization\n",
    "\n",
    "If you can do this â†’ interview-ready.\n",
    "\n",
    "---\n",
    "\n",
    "##### 3ï¸âƒ£ Mandatory â€œDebug Thinkingâ€ Practice\n",
    "\n",
    "Interviewers LOVE this.\n",
    "\n",
    "Pick **one LLM fine-tuning scenario** and rehearse it:\n",
    "\n",
    "###### Scenario:\n",
    "\n",
    "> â€œYour fine-tuned LLM outputs repetitive, low-quality answers.â€\n",
    "\n",
    "Your mental checklist must be automatic:\n",
    "\n",
    "* Data quality (format, duplicates, instruction clarity)\n",
    "* Learning rate too high\n",
    "* Overfitting (too many epochs)\n",
    "* Decoding issue (temperature, top-p)\n",
    "* Tokenization mismatch\n",
    "\n",
    "Say this **in order**.\n",
    "This signals *ownership*.\n",
    "\n",
    "---\n",
    "\n",
    "##### 4ï¸âƒ£ Force Yourself to Talk (Critical)\n",
    "\n",
    "Silent studying will **fail you**.\n",
    "\n",
    "###### Do this:\n",
    "\n",
    "* Open a doc or voice note\n",
    "* Explain these **out loud**:\n",
    "\n",
    "  * Attention\n",
    "  * LoRA\n",
    "  * Why fine-tuning fails\n",
    "  * AdamW vs SGD\n",
    "\n",
    "If you hesitate â†’ thatâ€™s your weak spot. Fix only those.\n",
    "\n",
    "---\n",
    "\n",
    "##### 5ï¸âƒ£ Learn to Say â€œIt Dependsâ€ Correctly\n",
    "\n",
    "This is a senior-level signal.\n",
    "\n",
    "Bad answer:\n",
    "\n",
    "> â€œAdam is better.â€\n",
    "\n",
    "Good answer:\n",
    "\n",
    "> â€œIt depends on model size, batch size, and stability needs.â€\n",
    "\n",
    "Then name **one concrete factor**.\n",
    "\n",
    "Interviewers want **judgment**, not certainty.\n",
    "\n",
    "---\n",
    "\n",
    "##### 6ï¸âƒ£ What NOT to Do (Very Important)\n",
    "\n",
    "âŒ Donâ€™t re-derive backprop\n",
    "âŒ Donâ€™t memorize equations\n",
    "âŒ Donâ€™t chase exotic architectures\n",
    "âŒ Donâ€™t over-optimize metrics talk\n",
    "\n",
    "None of this gets you hired.\n",
    "\n",
    "---\n",
    "\n",
    "##### 7ï¸âƒ£ Final 60-Minute Drill (Before Interview)\n",
    "\n",
    "Do this exact order:\n",
    "\n",
    "1. Explain **attention** in 2 minutes\n",
    "2. Explain **LoRA** in 1 minute\n",
    "3. Explain **why fine-tuning fails** in 2 minutes\n",
    "4. Explain **trade-offs of decoding strategies**\n",
    "5. Answer:\n",
    "\n",
    "   > â€œHow would you improve performance without changing the model?â€\n",
    "\n",
    "If you can do this calmly â†’ you are *absolutely* hireable.\n",
    "\n",
    "---\n",
    "\n",
    "##### The Truth (Coach Mode)\n",
    "\n",
    "Companies hire people who:\n",
    "\n",
    "* Donâ€™t panic when models misbehave\n",
    "* Know which knob to turn first\n",
    "* Can explain decisions clearly\n",
    "\n",
    "Youâ€™re not trying to be brilliant.\n",
    "Youâ€™re trying to be **reliable under pressure**.\n",
    "\n",
    "If you want, I can:\n",
    "\n",
    "* Simulate a **real interview** (Iâ€™ll push you)\n",
    "* Turn this into a **1-page interview cheat sheet**\n",
    "* Drill you with **failure-mode questions only**\n",
    "\n",
    "Tell me which one.\n"
   ],
   "id": "68bf47dd427d53e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## NN Basics",
   "id": "c5ae836aef555278"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* `Layer`: Layer is a group of neurons that process the input at the same stage\n",
    "* `Weights`: Learned parameters that determine how important each input is.\n",
    "* `bias`: It is constant that is added to weighted sum\n",
    "* `Activation Function`: **Allows the neural networks to understand non linearity** in the data allowing to learn and represent complex data patterns. If no activation function it would collapse into **single linear model**."
   ],
   "id": "1b4e3e62e3ef9812"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Activation Functions\n",
    "Activation Functions are mathematical functions applied to neuron's output in a neural network. They decide how much of signal passes forward.\n",
    "\n",
    "> Apart from introducing non-linearity it also controls the output of neurons in neural network"
   ],
   "id": "c024528e36be791c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Common Activation Functions are\n",
    "1. **Sigmoid**\n",
    "- $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $\n",
    "- Output range - (0 , 1)\n",
    "- Binary Classification output layers.\n",
    "- Problems are Vanishing gradients, Outputs are not zero centered\n",
    "\n",
    "2. **Tanh**\n",
    "- Output range: (-1, 1)\n",
    "- Zero Centered better than sigmoid. Strong gradients\n",
    "- Problem: vanishing gradient\n",
    "\n",
    "3. **Softmax**\n",
    "- Converts a vector of scores into probabilities that sums upto 1.\n",
    "- Used for multiclass classification output layers.\n",
    "\n",
    "4. **ReLU**\n",
    "- $ f(x) = max(0, x)$\n",
    "- Output range: [0, inf)\n",
    "- Fast computation, sparse activations, avoid vanishing gradients.\n",
    "- Problem: Dying ReLU - stucks at 0\n",
    "\n",
    "5. **leaky ReLU**\n",
    "- Fixes above problem by allowing small negative slope\n",
    "\n",
    "**How to choose (rule of thumb)**\n",
    "* Hidden layers: ReLU or Leaky ReLU\n",
    "* Binary classification output: Sigmoid\n",
    "* Multi-class classification output: Softmax\n",
    "* Regression output: No activation or linear"
   ],
   "id": "2375f1cc4d079a26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T01:02:18.088569900Z",
     "start_time": "2026-02-05T01:02:17.996813300Z"
    }
   },
   "cell_type": "markdown",
   "source": "#### Questions for Activation Functions",
   "id": "b2db8e695c3092b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T01:02:18.229828200Z",
     "start_time": "2026-02-05T01:02:18.096508900Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "##### 1ï¸âƒ£ Easy â€” Fundamentals (warm-up)\n",
    "\n",
    "**Question:**\n",
    "\n",
    "> What is an activation function, and why canâ€™t we train a deep neural network without one?\n",
    "\n",
    "\n",
    "##### 2ï¸âƒ£ Medium â€” Practical choice\n",
    "\n",
    "**Question:**\n",
    "\n",
    "> Why are Sigmoid and Tanh rarely used in hidden layers of modern deep networks, while ReLU and its variants are preferred?\n",
    "\n",
    "\n",
    "##### 3ï¸âƒ£ Medium-Hard â€” Output layers & loss coupling\n",
    "\n",
    "**Question:**\n",
    "\n",
    "> Why is Softmax almost always paired with cross-entropy loss in multi-class classification?\n",
    "> What would break if we used MSE instead?\n",
    "\n",
    "##### 4ï¸âƒ£ Hard â€” LLMs & fine-tuning context\n",
    "\n",
    "**Question:**\n",
    "\n",
    "> Modern LLMs (e.g., GPT-style models) do not use ReLU. Why are GELU or Swish preferred over ReLU in Transformers?\n",
    "\n",
    "\n",
    "\n",
    "##### 5ï¸âƒ£ Very Hard â€” Agentic AI & failure analysis\n",
    "\n",
    "**Question:**\n",
    "\n",
    "> Youâ€™re fine-tuning an LLM for an agentic system that performs tool calling and planning.\n",
    "> After fine-tuning, the agent becomes overly confident and collapses to repetitive actions.\n",
    "> How could activation functions or output activation behavior contribute to this, and how would you diagnose it?\n"
   ],
   "id": "d41fc786dd0da568"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T01:02:33.088247700Z",
     "start_time": "2026-02-05T01:02:32.992500700Z"
    }
   },
   "cell_type": "code",
   "source": "-0.35 * 0.45",
   "id": "3d1f1efd379afa60",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1575"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T01:03:38.662441500Z",
     "start_time": "2026-02-05T01:03:38.572297600Z"
    }
   },
   "cell_type": "code",
   "source": "0.275 - 0.1575 + 0.15",
   "id": "4e543f1e329639c8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2675"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "24120e47a14bffdb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
