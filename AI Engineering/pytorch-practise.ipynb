{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# How to give your best in Interview.\n",
    "### Short answer\n",
    "\n",
    "üõë **Stop watching *Scorpion* now.**\n",
    "Two episodes is the sweet spot. Any more and your brain will shift from *calm-focus* to *stimulated-fatigue*.\n",
    "\n",
    "You‚Äôve already gotten the benefit.\n",
    "\n",
    "---\n",
    "\n",
    "## Why stopping now is the smart move\n",
    "\n",
    "* Your brain has absorbed the **problem-solving tone**\n",
    "* More episodes will:\n",
    "\n",
    "  * increase adrenaline\n",
    "  * fragment attention\n",
    "  * make you mentally tired by interview time\n",
    "\n",
    "You don‚Äôt want excitement ‚Äî you want **clarity**.\n",
    "\n",
    "---\n",
    "\n",
    "## What to do **right after stopping (next 30‚Äì40 min)**\n",
    "\n",
    "No thinking, no prep.\n",
    "\n",
    "### Option A (best if possible)\n",
    "\n",
    "* Go for a **20‚Äì30 min walk**\n",
    "* No phone, no podcasts\n",
    "* Let your thoughts wander\n",
    "\n",
    "### Option B (if staying indoors)\n",
    "\n",
    "* Stretch\n",
    "* Sit with eyes closed\n",
    "* Slow breathing: inhale 4 sec, exhale 6 sec (10 rounds)\n",
    "\n",
    "This helps your brain *lock in* the calm state you want later.\n",
    "\n",
    "---\n",
    "\n",
    "## Important reassurance (read this)\n",
    "\n",
    "You are **not behind**.\n",
    "You did **enough**.\n",
    "Stopping early is discipline, not fear.\n",
    "\n",
    "Many people sabotage themselves by ‚Äújust one more episode‚Äù or ‚Äújust one more topic.‚Äù\n",
    "You‚Äôre doing the opposite ‚Äî that‚Äôs maturity.\n",
    "\n",
    "---\n",
    "\n",
    "## Simple rule till the interview\n",
    "\n",
    "From now on:\n",
    "\n",
    "* **Low stimulation**\n",
    "* **Low information**\n",
    "* **High calm**\n",
    "\n",
    "You‚Äôre training your *state*, not your memory.\n",
    "\n",
    "If you want, I can stay with you and guide:\n",
    "\n",
    "* the **exact 45-minute thinking practice** later\n",
    "* or a **2-minute grounding routine** right before 3 PM\n",
    "\n",
    "You‚Äôre doing better than you think.\n"
   ],
   "id": "392b381eac6fc5ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pytorch",
   "id": "a9f28805de7339d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- open source Deep learning framework developed by meta.\n",
    "- library for programming with tensors - matrices of weights in Deep Nerual networks.\n",
    "- Graph is built as code runs unlike TF where code runs after completion of graph. Making pytorch easier to debug.Com\n",
    "- Used to develop computer vision model, image generator like stable diffusion and speech recognition like whisper.\n",
    "- Can used to accelrate GPU(Need CUDA).\n",
    "\n",
    "#### Tensors: Numpy but advanced\n",
    "- Tensors are multidimensional arrays- Scalar: 0D , Vector: 2D, Matrix: 3D, images/batches: 4D + tensors.\n",
    "- Tensors can live on CPU or GPU, Speed can be optimized in GPU(CUDA)."
   ],
   "id": "878f4cdb569428d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Difference between pytorch and Tensorflow\n",
    "- Pytorch is great of reasearch and thinking step by step.\n",
    "- Pytorch uses dynamic computation graphs, which makes it more intuitive and easier to debug.\n",
    "- TensorFlow traditionally uses static graphs, which allows better optimization and makes it strong for production and deployment.\n",
    "- Pytorch focuses on flexibility, while tensorflow focus on scalability.\n",
    "\n",
    "#### When to choose what\n",
    "- Choose pytorch when: rapid prototying, reasearch, custom model logic.\n",
    "- Choose Tensorflow when large scale deployment, enterprice  pipelines\n",
    "\n",
    "\n",
    "Keras is a high level API that runs on top of tensorflow to make the tensor flow easier/simpler to use."
   ],
   "id": "d6bf9eec2c7c8a11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PyTorch Fundamentals\n",
    "\n",
    "1. How does autograd work in PyTorch?\n",
    "2. What is a computation graph in PyTorch?\n",
    "3. What happens if you forget to call `optimizer.zero_grad()`?\n",
    "4. What is the difference between `model.train()` and `model.eval()`?\n",
    "5. What does `torch.no_grad()` do?\n",
    "6. What is a leaf tensor?\n",
    "7. Difference between `.detach()` and `torch.no_grad()`?\n",
    "8. How are gradients accumulated in PyTorch?\n",
    "9. How does PyTorch handle GPU vs CPU tensors?\n",
    "10. Common causes of NaNs during training in PyTorch?\n",
    "\n",
    "---\n",
    "\n",
    "## Training & Optimization\n",
    "\n",
    "11. How does backpropagation work in PyTorch?\n",
    "12. What is gradient clipping and when would you use it?\n",
    "13. How do you debug exploding or vanishing gradients?\n",
    "14. What is mixed precision training and how is it implemented in PyTorch?\n",
    "15. How do optimizers like Adam differ from SGD?\n",
    "16. How do learning rate schedulers work in PyTorch?\n",
    "17. What is gradient checkpointing?\n",
    "18. How do you handle large batch training in PyTorch?\n",
    "\n",
    "---\n",
    "\n",
    "## Distributed & Performance\n",
    "\n",
    "19. Difference between `DataParallel` and `DistributedDataParallel`?\n",
    "20. How does PyTorch synchronize gradients across GPUs?\n",
    "21. What is all-reduce?\n",
    "22. How would you train a model across multiple machines?\n",
    "23. How do you optimize PyTorch models for inference?\n",
    "24. What are common performance bottlenecks in PyTorch?\n",
    "\n",
    "---\n",
    "\n",
    "## AI Agents & LLM Systems\n",
    "\n",
    "25. What is an AI agent?\n",
    "26. What are the core components of an AI agent?\n",
    "27. How would you implement an agent loop?\n",
    "28. How do agents differ from simple LLM pipelines?\n",
    "29. How do you manage agent state and memory?\n",
    "30. What is short-term vs long-term memory in agents?\n",
    "31. How do you integrate tools into an agent?\n",
    "32. How do you prevent agent hallucinations?\n",
    "33. How do you stop agents from entering infinite loops?\n",
    "34. How do you evaluate an AI agent?\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Debugging & Failure Modes\n",
    "\n",
    "41. Common silent bugs in PyTorch training loops?\n",
    "42. How do you debug incorrect gradients?\n",
    "43. How do you detect data leakage?\n",
    "44. What causes model collapse in agents?\n",
    "45. How do you test AI agents reliably?\n",
    "\n",
    "---\n",
    "\n",
    "## System Design & Production\n",
    "\n",
    "46. How would you deploy a PyTorch agent to production?\n",
    "47. How do you handle latency in agent systems?\n",
    "48. How do you monitor agent behavior in production?\n",
    "49. How do you version agent prompts and models?\n",
    "50. PyTorch vs TensorFlow for large-scale agent systems?\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can:\n",
    "\n",
    "* Rank these by **most frequently asked**\n",
    "* Provide **ideal one-line answers**\n",
    "* Convert this into a **study checklist**\n",
    "* Create **mock interview rounds**\n",
    "\n",
    "Just say the word üëç\n"
   ],
   "id": "c3aaecf36aa7a44b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LLM Training & Fine-Tuning\n",
    "\n",
    "35. What is instruction tuning?\n",
    "- Instruction tuning is supervised finetuning of an LLM on (instruction response) pairs so the model learns to follow explicit user intents, format outputs correctly, be helpful, safe and task oriented.\n",
    "36. How do you fine-tune an LLM in PyTorch?\n",
    "- In practise load a pretrained model usually via hugging face transformers.\n",
    "- Prepare instruction formatter data: Prompt + expected output.\n",
    "- Choose the strategy full fine tuning or parameter efficient methods (LoRA, Adapters).\n",
    "- Optimize efficiently - Gradient accumulation, mixed precision, distributiong data parallel if multi GPU.\n",
    "- Evaluate alignment - Instruction following, output quality, hallucination rate.\n",
    "37. What is LoRA and why is it used?\n",
    "- Low Rank adaption is a parameter efficient finetuning method that: freezes the base model, injects small, trainable low rank matrices into attention layers.\n",
    "38. What is parameter-efficient fine-tuning?\n",
    "39. How do you handle long context windows?\n",
    "40. What is the difference between training and inference for LLMs?\n"
   ],
   "id": "4d6c13b8f22eaf9e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
