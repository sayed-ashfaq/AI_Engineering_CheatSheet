{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ðŸ”µ Day 1 â€” Finetuning (LLMs + Speech)",
   "id": "7aab25d8db16f6d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Primary** 2:14 - 4 pm\n",
    "\n",
    "* Pretraining vs Finetuning vs Instruction tuning\n",
    "* LoRA / QLoRA / Adapters\n",
    "* When finetuning is a bad idea\n",
    "* Dataset curation & leakage\n",
    "* Eval after finetuning\n",
    "\n",
    "**Secondary**\n",
    "\n",
    "* NLP basics: tokenization, embeddings, perplexity\n",
    "* ML questions\n",
    "* AI related questions\n",
    "* System design questions\n",
    "\n",
    "**Hands-on**\n",
    "\n",
    "* Sketch finetuning pipeline on paper\n",
    "* Explain how you'd finetune a voice assistant for customer support\n",
    "\n",
    "**Must answer clearly**\n",
    "\n",
    "> â€œWhy finetuning instead of RAG here?â€\n"
   ],
   "id": "bee5fecbe079756a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Primary Resource for Finetuning - Skimming( Chap 7 & 9(inference))",
   "id": "321ba0dd845ac7ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### What FineTuning is?\n",
    "Finetuning: Finetuning is **taking a pre-trained model and further/continue training the model with your own data** so that it behaves the way you want for a specific task or Domain.\n",
    "- It can imporve the model's domain specific capabilities such as coding, medical question answering and can also strenghthen its safety.\n",
    "- Often used to imporve the model's **instruction following capabilities**. Particulary to ensure it follows **specific output style or format**\n",
    "- It is a process that happens after pretraining.\n",
    "- It is also possible to finetune a model to extend it's context length. (pg no: 600 in AI Engg chiphuyen)\n",
    "\n",
    "### Techniques to do Finetuning while considering memory requirements.\n",
    "1. Parameter Efficient FineTuning (PEFT) - Memory efficient technique that is used mostly.\n",
    "2. Adapter Based techniques\n",
    "3. Transfer Learning\n",
    "- Transfer learning is reusing pretrained model's learned capabilities for a new task with less data, time and compute.\n",
    "- Transfer Learning is machine learning technique where a trained model for one task is repurposed as foundation for the second task.\n",
    "4. Reinforcement Learning\n",
    "\n",
    "### Reasons to FineTuning a model\n",
    "Finetuning a model significantly need more resources not just in data and hardware but also in ML Talent.\n",
    "1. Primary reason to finetuning is to improve the model's quality in terms of capabilities to specific task and output formatting.\n",
    "2. Finetuning for Bias mitigation. Equalizing female CEO's and Male CEO to reduce model's gender bias.\n",
    "3. Distillation: Developing a small model so that it can imitate the behviour of larger model using data generated by larger model.\n",
    "4. A smaller model that is finetuned for specific task can outperform a larger model on that task.\n",
    "\n",
    "### Challenges in Finetuning\n",
    "You can improve model's capabilities by carefully crafted context and prompts as well. **Finetuning** a model for specific task can degrade the performance for other tasks.\n",
    "\n",
    "1. First, you need data. Annotated data can be slow and expensive to acquire manually, especially for tasks that demand critical thinking and domain expertise.\n",
    "2. Second, finetuning requires the knowledge of how to train models. You need to evaluate base models to choose one to finetune.\n",
    "3. Third, once you have a finetuned model, youâ€™ll need to figure out how to serve it. (Will you host yourself or Serve as API).\n",
    "4. More importantly, you need to establish a policy and budget for monitoring, maintaining, and updating your model.\n",
    "\n",
    "> As you iterate on your finetuned model, new base models are being developed at a rapid pace. These base models may improve faster than you can enhance your finetuned model.\n",
    "> * **If a new base model outperforms your finetuned model on your specific task, how significant does the performance improvement have to be before you switch to the new base model?**\n",
    "> * **What if a new base model doesnâ€™t immediately outperform your existing model but has the potential to do so after finetuningâ€”would you experiment with it?**\n",
    "\n",
    "\n",
    "### FineTuning and RAG\n",
    "Whether to use RAG or finetune a RAG depends on the problem whether your model's failure are instruction based or behaviour based.\n",
    "\n",
    "When to use **RAG**:\n",
    "- when you are facing Information based failures, model gives wrong and outdated facts.\n",
    "- Model doesn't have private/internal data, Organizational specific knowledge.\n",
    "- Model cutoff before recent events.\n",
    "- Research suggests RAG > finetuning for event based queries.\n",
    "- RAG system can help mitigate your model's hallucination.\n",
    "\n",
    "When to use **FineTuning**\n",
    "- If you are facing Behaviour based problem such as facts are correct but response is not fulfilling the task.\n",
    "- Output is irrelevant or shallow\n",
    "- Output format is not following\n",
    "- The model is not following the slang/style or level of detail.\n",
    "- Finetune can potentially help to mitigate the hallucinations with high quality data. But it can also worsen hallucinations if the data quality is low.\n",
    "> **In short, finetuning is for form/behaviour, and RAG is for facts.**"
   ],
   "id": "44bf4698250fe0a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Memory Requirements\n",
    "Key takeaways for memory bottle neck\n",
    "- The memory needed for finetuning is much higher thatn the memory needed for inference.\n",
    "- The key contributors for memory complexity are model's parameters and trainable parameters and it's numerical representations.\n",
    "- Quantization refers the practise of models from a format with more bits to a format with fewer bits.\n",
    "- Inference is typically doen using as few bits as possible, such as 16 bits, 8 4 bits.\n",
    "- Training is more sensitive to numerical precision so it's harder to train a model in lower precision.\n",
    "\n",
    "### BackPropagation and Trainable parameters\n",
    "A key factor that determines a model's memory footprint during finetuning is it's number of trainable parameters.\n",
    "- Neural networks are trained using method called BackPropagation.\n",
    "- It consists of two phases\n",
    "    1. Forward pass: The process of computing the output from the input. executed in both training and inferencing.\n",
    "    2. Backward pass: the process of updating weights using the aggregated signals from the forward pass. Executed only in training.\n",
    "#### BackPropagation\n",
    "1. Compare the computed output from a forward pass with the output label. The difference between computed o/p and expected o/p is called as **loss**.\n",
    "2. Compute how much each trainable parameter contributes to the mistake. the value is called gradient. Derivatives of weights.\n",
    "3. Adjusted the parameter values using their corresponding gradient. How much each parametr should be adjusted is determined by optimizer such SGD. Stochastic Gradient Descent or Adam. (For transformer usually Adam).\n",
    "\n",
    "### How much memory worth of hardware is needed. - Memory Math\n",
    "- The things in model that contribute in memory requirement are\n",
    "    1. Number of parameters(N)\n",
    "    2. Memory needed for each parameter (M)\n",
    "    3. The memory for activation. (We need extra memory Usually 20% of model's weights) `N x M + 0.2(N x M)`\n",
    "\n",
    "For example: Consider a 13B parameter model with 2 bytes the model's weights which will require 13B x 2 = 26 GB. And the total memory for inference will be 26 x 1.2 = 31.2 GB\n",
    "- As the models get bigger it requires greater memery. for a 70b model with 2 byte of weight need 140 gb just of weights.\n",
    "- Over all the memory needed for training is calculated as: <br>\n",
    "$$ Training memory = model weights + activations + gradients + optimizer states. $$\n",
    "\n",
    "> **Gradient Checkpoint or activation computation:** Gradient checkpointing reduces training memory by saving only selected activations during the forward pass and recomputing the rest during back propagation. This reduces the memory usage.\n"
   ],
   "id": "aa46b2596adc093"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Learning Finetuning by pipeline\n",
    "1. Data Engineering -> Later\n",
    "2. Memory Requirements\n",
    "3. Quantization\n",
    "4. Techinques\n",
    "5. Finetuning hyperparameters\n",
    "6. Evaluation\n",
    "7. Inferencing cheaper and faster."
   ],
   "id": "b5f7c1d40a29e00e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Quantization\n",
    "Quantization converts model weights from high precision to low precision to improve inference efficiency.\n",
    "- A model 10B parameters with weights of 32 bit format requires 40GB of memory whereas same model with 16 bit format requires 20 GB memory.\n",
    "- What does 32 bit format contains 1 bit -> sign | 8 bit -> exponent | 23 bits Mantissa\n",
    "- **What to quantize:** weights quantization is more common than activation quantization.\n",
    "- **When to quantize:** Quantization can happen during training or post training. **Post Training Quantization** is by far the most common.\n",
    "\n",
    "**Inference Quantization**\n",
    "- Reduced precision not only reduces the memory footprint but also often improves computation speed.\n",
    "- Allows larget batch size enabling the model to process more inputs in parallel.\n",
    "- Reduced precision speeds up computation, which further reduces **inference latency** and **training time.**\n",
    "\n",
    "**Training Quantization**:\n",
    "* FP32 -> Also called as FullPrecision"
   ],
   "id": "4ccf3f8c30abce31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### FineTuning Techniques:\n",
    "* Since we have learnt how much it would cost to finetune a model. Higher the the more memory finetuning requires the fewer people who can afford to do it. That's why we use techniques that reduces memory footprint and allows us to adapt models.\n",
    "### PEFT(Parameter Efficient FineTuning) - LoRA, QLoRA.\n",
    "#### LoRA\n",
    "* Full finetuning updates all weights in an llm - Which requires high computational power, Huge GPU memory, slower training, Highly expensive.\n",
    "* For that LoRA says what if we don't change the weights at all, instead learn a small corrective updates by adding fewer weights.\n",
    "* If we take any llm it already knows language, how to talk and how to perform certain actions -> And we modify the behaviour to perform specific tasks with higher accuracy.\n",
    "\n",
    "**Equation**:\n",
    "* **W** = weights and biases of original model\n",
    "* $ \\hat(W) = W + \\Delta(W)$ # new weights\n",
    "* $ \\Delta(W) = BA $\n",
    "\n",
    "**When NOT to use LoRA:**\n",
    "* LoRA struggles when you need new knowledge (not style/behavior).\n",
    "* Domain shift is huge\n",
    "* You need structural changes\n",
    "#### QLoRA\n",
    "LoRA finetuning but on top of 4 bit quantized frozen base model with high precision adapters.\n",
    "* Only the base model is quantized to Int4 and we will froze them\n",
    "* On top of that We will add LoRA matrices F16/BF16\n",
    "* In this model gradient never touches the Int4 weights\n",
    "* In summary, QLoRA enables finetuning massive LLMs on limited hardware by combining 4 bit quantizaiton for storage with LoRA low rank high precision updates."
   ],
   "id": "96908d206e9ac8b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### KeyWords and it's definitions\n",
    "1. **Self-supervised finetuning** is adapting a pretrained model using automatically generated labels from the data itself, without human annotation, to improve performance on a domain or task.\n",
    "2. Optimizer in realtime to Finetuning\n",
    "3. Learning rate\n",
    "4. Semantic Parsing\n",
    "5. Batch Normalization\n",
    "6. Quantization: Quantization converts model weights from high precision to low precision to improve inference efficiency.\n",
    "7."
   ],
   "id": "1e0c50d39e270a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a14bca9ea20e091f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
