{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Day 1",
   "id": "33ed94ec099ae4ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. **Easy** — What is the difference between *training* and *inference* in an AI system? Give one real-world example.\n",
    "\n",
    "2. **Easy–Medium** — What problem does an *embedding* solve in AI systems, and where are embeddings typically used in production?\n",
    "\n",
    "3. **Medium** — Explain how a basic **RAG (Retrieval-Augmented Generation)** pipeline works end to end. Where can it fail?\n",
    "\n",
    "4. **Medium–Hard** — You are serving an LLM-based API. Latency suddenly spikes under load. What are the *first 3 things* you would inspect, and why?\n",
    "\n",
    "5. **Hard** — You need to design a **scalable, cost-efficient AI agent system** that:\n",
    "\n",
    "   * Uses tools (APIs, DBs)\n",
    "   * Maintains short-term and long-term memory\n",
    "   * Avoids hallucinations\n",
    "\n",
    "   Describe the **architecture**, key components, and trade-offs.\n"
   ],
   "id": "66e68def430f72ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Easy\n",
    "- **Training** an AI system involves - Cleaning the data, model selection, training the model using the data so that model learns the patterns from the data.\n",
    "- Inference involves getting output from the model that has been trained.\n",
    "\n",
    "**Correct Answer**<br>\n",
    "- Training is the phase where model parameters are optimized using labeled or unlabeled data via a loss function and backpropagation.\n",
    "- Inference is the phase where the trained, frozen model generates predictions on new data with no learning involved.\n",
    "---\n",
    "2. Easy - Medi\n",
    "- Embeddings are numerical representation of data that has semantic meaning.\n",
    "- In production, Whenever a model wants to search information related to something embeddings are really helpful\n",
    "\n",
    "**Correct Answer**<br>\n",
    "- Embeddings map unstructured data into dense vectors such that semantic similarity becomes measurable using distance metrics.\n",
    "- In production, they’re used for semantic search, RAG, recommendations, deduplication, and clustering via vector databases.\n",
    "---\n",
    "3. Medium\n",
    "- RAG pipeline in short\n",
    "- Documents -> document-loader -> chunks -> embeddings -> stored in database\n",
    "- User Query -> convert into embeddings -> bi encoder compares query and documents to retrieve the relevant documents\n",
    "- retrieved documents + user query + prompt => sent to llm => llm generate response based on information and prompt.\n",
    "\n",
    "**Correct Answer**<br>\n",
    "##### **What’s missing**\n",
    "* Failure modes (explicitly asked).\n",
    "* No mention of chunk size, retrieval quality, or context window limits.\n",
    "##### **Key failures interviewers expect**\n",
    "* Poor chunking → lost context\n",
    "* Bad embeddings → irrelevant retrieval\n",
    "* Retrieval misses critical docs\n",
    "* Context overflow → truncation\n",
    "* LLM hallucination despite correct retrieval\n",
    "---\n",
    "4. Medium - Hard\n",
    "- Check the llm api calls whether they are looping or not\n",
    "- Check logs\n",
    "\n",
    "**Correct Answer**<br>\n",
    "##### Expected top 3 checks\n",
    "* LLM provider latency / rate limiting (timeouts, retries, cold starts)\n",
    "* Embedding + vector DB latency (ANN index type, cache misses)\n",
    "* Concurrency & scaling (async vs sync, worker saturation, request queuing)\n",
    "> Red flag: Not thinking in terms of systems, infrastructure, and bottlenecks.\n",
    "---\n",
    "5. Hard\n",
    "pass\n",
    "### Interview-ready answer (short, crisp, no fluff)\n",
    "\n",
    "> **Question:** Design a scalable, cost-efficient AI agent system with tools, memory, and low hallucination.\n",
    "\n",
    "**Correct Answer:**\n",
    "\n",
    "I’d design it as a **modular agent architecture**:\n",
    "\n",
    "1. **Planner (LLM-based)**\n",
    "   Interprets user intent and breaks it into steps. This reduces random tool calls and keeps execution structured.\n",
    "\n",
    "2. **Tool Executor Layer**\n",
    "   Controlled interface for APIs, databases, and services. Tools are schema-validated and permissioned to avoid unsafe or unnecessary calls.\n",
    "\n",
    "3. **Memory System (2 layers)**\n",
    "\n",
    "   * **Short-term memory:** Conversation context stored in-session (sliding window or summary).\n",
    "   * **Long-term memory:** Vector DB + metadata for facts, preferences, and past outcomes, retrieved via embeddings.\n",
    "\n",
    "4. **Retrieval + Grounding (Anti-hallucination)**\n",
    "   Use RAG for any factual queries. The agent is forced to answer *only* from retrieved context, with fallback to “I don’t know” if confidence is low.\n",
    "\n",
    "5. **Guardrails & Validation**\n",
    "\n",
    "   * Output validation (JSON schema, regex)\n",
    "   * Tool call verification\n",
    "   * Confidence thresholds on retrieval\n",
    "\n",
    "6. **Scalability & Cost Control**\n",
    "\n",
    "   * Async execution\n",
    "   * Caching embeddings and tool results\n",
    "   * Smaller models for planning, larger ones only for final response\n",
    "\n",
    "**Trade-offs:**\n",
    "More components increase complexity, but you gain reliability, lower hallucinations, and predictable costs—mandatory for production agents."
   ],
   "id": "7b13c00e42b0d71b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Day 5",
   "id": "3e910f86f917b536"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Easy\n",
    "\n",
    "**What is the difference between a *prompt* and a *system prompt* in an LLM-based application?**\n",
    "Why does this distinction matter in production?\n",
    "\n",
    "- The prompt is the instructions or query given by the user, it could be differnt for every query. Where as System prompt is defines the character/persona of the llm on how it should behave or think. It is almost same for throughout the conversation.\n",
    "- The distinction help the model to think like specific persona, system prompt is usually given by the developer that make the model think in particular way.\n"
   ],
   "id": "a8192d4ee9da6b77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Easy–Medium\n",
    "\n",
    "**Why do most production LLM systems avoid sending the entire conversation history to the model every time?**\n",
    "What strategies are used instead?\n",
    "\n",
    "- llm comes with context length which it can able to process, when the entire conversation history is sent to model everytime, the context length increase which consumes more tokens and the model's ability to reasoning decreases which causes more generalised or hallucinations.\n",
    "- TO overcome this instead of entire conversation you can send summary of conversation history that tell intent of the conversation. or Store converstation history in the vector database and using RAG pull out the messages or limit the previous chat instead of entire history only last 10 chats.\n",
    "- But among all i feel summarizing the chat history is the best option because it explicitly says the intent of the conversation."
   ],
   "id": "c06aa09fd6fea063"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Medium\n",
    "\n",
    "**You are building a customer-support AI assistant.**\n",
    "How would you prevent it from:\n",
    "\n",
    "* hallucinating answers\n",
    "* giving outdated information\n",
    "\n",
    "Explain your approach at a system level.\n",
    "\n",
    "- First identify the cause of hallucinations such as whether is it from underspecified prompt or lack of information.\n",
    "- If lack of information then i would implement RAG technique to provide relevant information to query and sent it combined to llm and explicitly give instruction to answer the query based on context.\n",
    "- I will upload the data such as FAQs, policies, resolved queries to the vector database.\n",
    "- With this method not only reduced hallucinations but also the data will be latest and up to date.\n",
    "\n",
    "---\n",
    "\n",
    "I’d ground the model using **RAG** with a trusted, up-to-date knowledge base.✅\n",
    "\n",
    "At a system level:\n",
    "\n",
    "* Retrieve relevant documents at inference time\n",
    "* Instruct the model to **answer only from retrieved context**\n",
    "* Add a fallback: *if information isn’t found, say “I don’t know”*\n",
    "\n",
    "To prevent outdated info:\n",
    "\n",
    "* Keep the knowledge base **continuously updated**\n",
    "* Version documents and monitor retrieval quality\n",
    "\n",
    "This reduces hallucinations and ensures responses stay accurate and current.\n",
    "\n",
    "---\n"
   ],
   "id": "38f276c44874eea2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Medium–Hard\n",
    "\n",
    "**An LLM agent is allowed to call tools (APIs, DBs).**\n",
    "How do you ensure:\n",
    "\n",
    "* tool calls are correct\n",
    "* the agent doesn’t loop or misuse tools\n",
    "\n",
    "What controls would you put in place?\n",
    "\n",
    "**Answer**:\n",
    "- To evaluate the tools calls are correct i would use frame works such as langsmith, deepeval. I will prepare a dataset using edgecases that would trigger the tool call. I would mention tool name and description for llm.\n",
    "- I will evaluate them based on that.\n",
    "- Then to avoid agent loop i would implement gaurdrails such as rate limiting which breaks the loop if there were many, and implement a layer which validates the tools, if there is some execution layer then i would add human in the loop mechanism.\n",
    "\n",
    "With these methods the llm agent will be robust make proper tool calls, and doesn't break.\n",
    "\n",
    "---\n",
    "\n",
    "To ensure **correct tool calls**:✅\n",
    "\n",
    "* Define strict **tool schemas and descriptions**\n",
    "* Evaluate tool usage offline with test cases and edge cases\n",
    "* Log and monitor tool-call accuracy in production\n",
    "\n",
    "To prevent **loops or misuse** at runtime:\n",
    "\n",
    "* Enforce **max tool calls per request**\n",
    "* Add **timeouts and rate limits**\n",
    "* Validate tool inputs/outputs before execution\n",
    "* Define explicit **stop conditions**\n",
    "* Use **human-in-the-loop** for high-risk actions\n",
    "\n",
    "This ensures tools are used **correctly, safely, and predictably** in production.\n",
    "\n",
    "---\n"
   ],
   "id": "6565330bd227153b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Hard\n",
    "\n",
    "**Design a multi-tenant AI platform** where:\n",
    "\n",
    "* thousands of users share the same LLM backend\n",
    "* data privacy between users is guaranteed\n",
    "* costs stay predictable\n",
    "\n",
    "Explain the **architecture, isolation strategy, and cost controls**.\n",
    "\n",
    "Here’s a **short, crisp, interview-style answer** (what a senior interviewer wants to hear):\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Hard — Multi-Tenant AI Platform (Interview Answer)**\n",
    "\n",
    "I’d use a **shared LLM backend** with a **tenant-aware application layer**.\n",
    "\n",
    "Each request carries a **tenant ID**, which is used to:\n",
    "\n",
    "* Route data to **tenant-isolated vector indexes / storage**\n",
    "* Enforce **access control and encryption** per tenant\n",
    "\n",
    "For **isolation**, I’d ensure:\n",
    "\n",
    "* Logical isolation at the data layer (separate namespaces / indexes)\n",
    "* Strict auth checks so prompts, embeddings, and logs never mix across tenants\n",
    "\n",
    "For **cost control**:\n",
    "\n",
    "* Per-tenant **rate limits and token quotas**\n",
    "* Request batching and response caching\n",
    "* Tiered plans (limits on context size, tools, and calls)\n",
    "\n",
    "This keeps inference shared and efficient, while **data privacy and costs remain predictable**."
   ],
   "id": "e33bb13cf0509c44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Day 8",
   "id": "5bd06d59b4a4366b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### 1. Easy (Real usage)\n",
    "\n",
    "You deploy an LLM-based chatbot. Users complain that answers are *sometimes correct, sometimes completely wrong*.\n",
    "**What are the first two things you would check, and why?**\n",
    "\n",
    "- I would check whether the chatbot is extracting information and telling on those based information or not, because llms hallucination rate is high when the key information to the query is missing.\n",
    "- Then if retriving informtion is not the probelm i would check the prompt and query whether they are alligned or not. Because if both query and prompt have different goal, llms gives inaccurate responses to fill the gap.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Easy–Medium (Production reality)\n",
    "\n",
    "Your RAG system works well in testing, but in production users say:\n",
    "\n",
    "> “It answers fast, but often misses obvious information from the documents.”\n",
    "\n",
    "**What are the likely causes, and how would you debug this step by step?**\n",
    "\n",
    "- The most likely cause that misses obvious information in RAG system would be llm is not utilizing the information correctly. When we retrieve relevant information using sematic search it gives approximate results it may or may not be helpful. Since there are not much step in between that's the reason it is faster.\n",
    "- I would check how documents are being stored in the vector database, chunking strategy, retrival logic. I would try to implement reranking using cross encoder for more accurate responses but there would be some latency if that's not the priority.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Medium (System behavior)\n",
    "\n",
    "An AI agent that uses tools starts repeating the same tool calls and never finishes the task.\n",
    "\n",
    "**What could cause this behavior, and what concrete safeguards would you add to stop it?**\n",
    "\n",
    "- the agent uses tool to get information if there is none then it would go again and call the tool. This happens when the parameter could be wrong, tool details like name, description could be wrong that it wouldn't understand.\n",
    "- To avoid this behaviour i would add more details about the tools so that llm can understand clearly, I would implement gaurdrails such as rate limiting if it calls the same tools again then it would stop and break the loop and say the answer with something like i don't know.\n",
    "---\n",
    "\n",
    "### 4. Medium–Hard (Scaling & cost)\n",
    "\n",
    "Your AI application suddenly gets **10× traffic** after a product launch.\n",
    "Latency and cloud costs both spike.\n",
    "\n",
    "**What immediate actions do you take, and what long-term changes do you make to the system?**\n",
    "\n",
    "- In order to reduce the load on system or handle latency i would implement load balancers, so that load is spread across different servers.\n",
    "- I would do the horizontal scaling additionally\n",
    "- To control costs - I would use cache where same or most common questions are shared across. I would per hours based cloud system. so that even though there is huge traffic my per hour cost will be the same. but it is costlier than serverless,\n",
    "---\n",
    "\n",
    "### 5. Hard (Failure under pressure)\n",
    "\n",
    "A critical enterprise customer reports that:\n",
    "\n",
    "* the AI gave a **confident but wrong answer**\n",
    "* it used **their private data in another user’s response**\n",
    "\n",
    "This is a P0 incident.\n",
    "\n",
    "**Walk me through exactly what you would do in the first hour, and what system changes you’d implement to ensure this never happens again.**\n",
    "\n",
    "- The AI always gives confident answer even the answer is factually wrong as these are predicting models. So in order to reduce the wrong answer you have to provide relevant information and mention in the prompt that use the information to answer the question.\n",
    "- Since the private data is getting leaking and to avoid that\n",
    "    - I would avoid using it plain llms response rather I would implement nodes/layers that validate the response so that any senstive is not being going response.\n",
    "    - I would implement role based check where the session info which has user's information should match the exact user's info in the response.\n",
    "- with these techniques the AI system will be more reliable and accurate.\n"
   ],
   "id": "a0c9cc89434faf25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Day 15",
   "id": "19ea43091f0edcf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Below is a **new set of 5 interview questions on fine-tuning**, **easy → hard**, written from an **applied AI engineering** perspective (production, trade-offs, failures).\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Easy\n",
    "\n",
    "**What problem does fine-tuning solve that prompting or RAG cannot?**\n",
    "Give one concrete example.\n",
    "\n",
    "> Answer:\n",
    "- Finetuning is method of post training an LLM model so that it alligns to ther user. Modifying the behaviour and tone of LLM to user centric.\n",
    "- Prompting is about giving the intructions to an LLM on how to act, respond for the query, how to output the response.\n",
    "- RAG is providing the information to the LLM where it uses that to answer the query.\n",
    "For example: In Healthcare finetune a model so that it behaves and talks like a doctor, where as RAG provide extra information if needed to answer the query. Prompt gives the instructions to the llm about its capabilities.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Easy–Medium\n",
    "\n",
    "You fine-tune an LLM and observe that:\n",
    "\n",
    "* training loss goes down\n",
    "* real user responses get worse\n",
    "\n",
    "**Why can this happen?**\n",
    "\n",
    "> Answer\n",
    "---\n",
    "\n",
    "#### 3. Medium\n",
    "\n",
    "You are deciding between **fine-tuning vs RAG vs prompt engineering** for a new product.\n",
    "\n",
    "**How do you decide which approach to use?**\n",
    "What signals push you toward fine-tuning?\n",
    "\n",
    "> Answer\n",
    "\n",
    "Identify the problem we are trying to solve, is it behaviour based or event based or actions based.\n",
    "- First off i will experiment with carefully crafted prompts to optimize the response. After maxing out prompting\n",
    "- If the model is hallucinating, response is factually incorrect then I would add information to the model using RAG.\n",
    "- If the model is giving the facts correctly but unable to perform or fullfil the task even i would go with finetuning.\n",
    "- If needed i would combine RAG to finetuned model along with prompt so that model would perform it's best to compplete the tasks.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Medium–Hard\n",
    "\n",
    "A fine-tuned model performs well in staging but fails in production with:\n",
    "\n",
    "* hallucinations\n",
    "* brittle behavior to small prompt changes\n",
    "\n",
    "**What went wrong, and how do you fix it?**\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Hard\n",
    "\n",
    "You must fine-tune an LLM for a **regulated domain** (finance / healthcare / legal) where:\n",
    "\n",
    "* training data is limited\n",
    "* mistakes are costly\n",
    "* explanations are required\n",
    "\n",
    "**Design the fine-tuning strategy, safety controls, and evaluation framework.**\n",
    "\n",
    "---\n",
    "\n",
    "Answer in order.\n",
    "I’ll evaluate like an experienced AI engineer who has shipped fine-tuned models to production.\n"
   ],
   "id": "3086f1ac475ad097"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T14:36:55.667849500Z",
     "start_time": "2026-02-05T14:36:55.612816100Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1a30b917d42aee06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Day 2 - Master Works interview preparation",
   "id": "c9c561e34450d007"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Easy\n",
    "\n",
    "**What makes an AI system “agentic” rather than just an LLM with tools?**\n",
    "Name the minimum components required.\n",
    "\n",
    "- Inside Agentic AI system, llms have capabilities to decide and perform certain actions. Yes they do have tools but they can decide when to use which tools, and reason whether the tool usage have given me enough information to respond.\n",
    "- It's just not llm with tools but intelligence that use tools to perform actions.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Easy–Medium\n",
    "\n",
    "An agent completes tasks correctly but is **slow and expensive**.\n",
    "\n",
    "**What are the first optimizations you would apply, and why?**\n",
    "\n",
    "- First, we need to understand why the agent is slow. The possible causes could be context overload, looping over unnecessarily.\n",
    "- You have start with simplest things to solve such as reducing the context - Context could be because of two reasons. Context can increase token usage for llm which might slow down the response and expensive.\n",
    "    - Prompt over engineering - Try to be more specific inside the prompt may be reduce the examples.\n",
    "    - Information overload - Try to summarize the information so that it get information and also doen't deviate from facts.\n",
    "    - The above approaches may reduce the latency and costs but possibly slightly degrade the performace of agent.\n",
    "    - In order to decrease the looping around layers of agent, add gaurdrails so that it loops for limited cycles if that doesn't provide enough information to respond then it may stop and give respone like \"I don't know\".\n",
    "\n",
    "Improvement:\n",
    "- Model tiering - Cheaper model for planning or simpler tasks and expensive for reasoning.\n",
    "- Tool call batching or caching\n",
    "- Early exit conditions\n",
    "- Reduce tool invocation frequency\n",
    "- Separte planning tokens vs execution tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Medium\n",
    "\n",
    "Your agent uses tools and memory, but users complain it:\n",
    "\n",
    "* forgets important details\n",
    "* repeats the same questions\n",
    "\n",
    "**What is broken in the system design, and how would you fix it?**\n",
    "\n",
    "Based on complaint i feel like the context window might have filled with the long conversations. So it is not taking previous recent conversation into consideration. causing repeating the same question and forgetting the important details.\n",
    "- In order to fix this, i would summary the memory to reduce the context windows but there could be some loss of information.\n",
    "- I would add the chathistory to vectordatabase so that it only pulls the necesary information related to the query. But you may need multiple RAG to retrive query related information and to retrieve previous chats.\n",
    "- And repeating the same question could also be cause of chache memory. So you can reduce the cache timelimit or make some optimization so that the questions is not repeated again.\n",
    "\n",
    "Imporvising\n",
    "- Include Working memory, episodic memory, Long term memory, retrieval policy.\n",
    "- Repeacting questions: No state grounding, no question ansewr confirmation memory, no deduplication strategy.\n",
    "---\n",
    "\n",
    "### 4. Medium–Hard\n",
    "\n",
    "An autonomous agent is allowed to take actions (send emails, update DBs, trigger workflows).\n",
    "\n",
    "**How do you prevent unsafe or irreversible actions while keeping the agent useful?**\n",
    "\n",
    "- To prevent unsafe or irreversible actions the best thing to do is implementing guardrails for critical actions.\n",
    "- Such as adding human in the loop before any action. But it would slow up the process since human is always needed.\n",
    "- If you want to be more autonomous with less human involvement then you can implement layers of llm where i would recheck  the response or actions it is going to perform, if there are any then change it again or break and log the action.\n",
    "\n",
    "Improvising\n",
    "- Action classification\n",
    "- dry run / preview model\n",
    "- Rate limits\n",
    "-\n",
    "---\n",
    "\n",
    "### 5. Hard\n",
    "\n",
    "Design a **multi-agent system** where:\n",
    "\n",
    "* agents collaborate on complex tasks\n",
    "* failures are isolated\n",
    "* costs remain predictable\n",
    "\n",
    "Explain the **coordination model, communication strategy, and safety controls**.\n",
    "> answer\n",
    "\n",
    "“I’d design a multi-agent system as a distributed system with explicit control, not autonomous agents.\n",
    "\n",
    "I’d use a central orchestrator to decompose a complex task into deterministic subtasks and assign each to a narrow, specialized agent. Agents don’t decide what to do next—they just execute their scoped responsibility and return structured output.\n",
    "\n",
    "For failure isolation, each agent runs behind a hard boundary with timeouts, token limits, and retries, so failures are contained and recoverable.\n",
    "\n",
    "Cost predictability comes from fixed workflows: a bounded number of agents, per-agent token budgets, and no uncontrolled agent-to-agent communication.\n",
    "\n",
    "The result is a boring but reliable system—collaborative, fault-tolerant, and with deterministic cost.”*\n"
   ],
   "id": "900710653743ab85"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Day 3 - Master Works",
   "id": "b1de159345699c52"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Easy (Behavior)\n",
    "\n",
    "An agent gives correct answers in isolation but fails when a task requires **multiple steps**.\n",
    "\n",
    "**What is missing in the system design?**\n",
    "\n",
    "I am assuming multiple steps means planning, sequencing and statetracking. The system is lacking planner and executer loop, explicit state/memory tracking and task decomposition.\n",
    "- Without these the agent can't reliably chain steps,\n",
    "* doesn't know what's done or decide the next.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Easy–Medium (Memory)\n",
    "\n",
    "Your agent remembers irrelevant details but forgets critical constraints given earlier.\n",
    "\n",
    "**What does this tell you about the memory design, and how would you fix it?**\n",
    "\n",
    "An agent has long term memory where knowledge database is stored, and short term memory usually context which is limited. If the agent remembers irrelevant details then it's context could be filled with irrelevant information. In order to mitigate this we can modify the context such as summarizing the chat history keeping critical constraints OR you can maintain a RAG for chathistory where it can retrieve data that is critical from past history.\n",
    "- With this the context space could be saved, cost effective and performance also increases\n",
    "* _**Separate preferences vs task constraints vs factual memory**_\n",
    "* **Add constraint pinning (always keep budget, rules, goals in system context)**\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Medium (Tool reliability)\n",
    "\n",
    "A tool used by the agent is slow, flaky, or returns partial results.\n",
    "\n",
    "**How should the agent reason and behave in this situation?**\n",
    "What system-level protections would you add?\n",
    "\n",
    "* Latency and partial information from tool will effecte the model's performance as well as user's trust.\n",
    "* so we need to design the agent to work underthis circumstances reliably. With partial information hallucination rates would increase, to mitigate them try to get additional information from other tools or say the user's that he doesn't have enough informtion to answer.\n",
    "* Cache the recent or often asked queries which will reduce the tools usage and increase the speed with accuracy.\n",
    "* with these methods the agent can answer user's query with reliability.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Medium–Hard (Autonomy control)\n",
    "\n",
    "You want to gradually increase an agent’s autonomy in production.\n",
    "\n",
    "**How do you stage this safely from “assistive” to “fully autonomous”?**\n",
    "\n",
    "* Initially i wouldn't give the agent to autonomy, i would implement the human in the loop guard rails for critical tasks. I would observe the failure by using logs and evalutaions metrics, failure patterns.\n",
    "* This will give the weak poitns of an agent, where it is giving 100% results and where it is underperforming, I would assess each task where agent is failing and try to rectify it. Gradually i would add more tasks to autonomy. i would strictly implement guardrails so that if the model fails it wouldn't crash the system. and remove the human involvement.\n",
    "* This would increase the agent autonomy while keeping the system safe and healthy.\n",
    "* **I would implement layers such as suggest -> approval -> execute**\n",
    "* Shadow run before real action, **_Give only necessary permission to perform tasks_**\n",
    "* **Kill swithces for emergencies.**\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Hard (Agent failure analysis)\n",
    "\n",
    "After deployment, you notice that agents succeed on easy tasks but fail catastrophically on rare edge cases.\n",
    "\n",
    "**How do you detect, analyze, and systematically reduce these failures over time?**\n",
    "\n",
    "* Add production monitoring for failure signals: tool errors, repeated loops, low-confidence retrieval, user corrections, and escalation rates.\n",
    "* Log full trajectories(plan -> tool calls -> outputs).\n",
    "* Cluster failures, run root cause analysis and build edge case evaluation dataset from real incidents.\n",
    "* Use fall back strategies(human escalation, safe refusal), improve guardrails (timeouts, circuit breakers, constraint checks).\n",
    "> Production agents improve through an incident → dataset → regression → guardrail loop, not one-time prompting\n"
   ],
   "id": "c0c1ea515e13807d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build safe and Secure Agents - Youtube IBM",
   "id": "6a51af2b5cfdd345"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Threats/ Challenges\n",
    "\n",
    "#### Security\n",
    "- Hijacking - Attacker manipulate tool calls, parameters or agent planning logic to call unintended APIs.\n",
    "- Prompt Injections - Make agents perform things that aren't not supposed to do. {Direct and indirect injection from hidden tools, documents, webpages, emails and so on}\n",
    "- DataPoisoning - infiltrating training data with unwanted response makes agent learns wrong, biased or malicious behaviour.\n",
    "- Infections - Software - Agent are only as secure as the software ecosystem around them.\n",
    "- Model Extraction - Attackers repeatedly query the model to  recontruct behaviour, steal IP, infer training data.\n",
    "- Evasion attacks - inputs crafted to bypass safety filters, content moderation, policy checks.\n",
    "\n",
    "#### Governance\n",
    "1. Autonomous or human intervention.  - Adding threshold\n",
    "    - Low risk - full automation,\n",
    "    - med risk - human review,\n",
    "    - high risk - Human approval only\n",
    "2. Bias  - There shouldn't be any bias or .\n",
    "3. Accountability - You need to track who approved this behaviour, which version made this decision,what data influenced\n",
    "\n",
    "### Mitigation strategies\n",
    "**Securing:**\n",
    "1. AISPM(AI security and Policy management) - Make sure that AI is following company policies strictly.\n",
    "    - Allowed tools, Allowed actions, prompt constraints, data access rules.\n",
    "2. GuardRails and Firewall protection\n",
    "    - Validate inputs\n",
    "    - Sanitize outputs\n",
    "    - Block sensitive data leaks\n",
    "    - Enforce schema and intent\n",
    "3. Penetration testing or Edge case testing.\n",
    "\n",
    "**Governance:**\n",
    "\n",
    "4. Life Cycle : Design -> development -> Testing -> Deployment -> monitoring -> retiring\n",
    "    - Version control of prompts, tools, policies, models\n",
    "5. Risk Assessment\n",
    "6. Evaluation and testing before going to production\n",
    "    - Safety benchmarks\n",
    "    - Bias test\n",
    "\n",
    "### Monitoring and dashboarding\n",
    "Dashboard should tracks\n",
    "- Tool usage anomolies\n",
    "- Policy violations\n",
    "- Hallucination rates\n",
    "- refusal rates\n",
    "- Latency and failure patterns\n",
    "- Human overrides frequency."
   ],
   "id": "7338cf213ac9a785"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Day 1 - Master Works(Round 3)",
   "id": "79c6be0d11ca0cfa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Easy (Planning Failure)\n",
    "\n",
    "Your agent answers correctly, but it often skips steps and jumps straight to the final response.\n",
    "\n",
    "**What does this tell you about the agent loop, and how do you fix it?**\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Easy–Medium (Memory Explosion)\n",
    "\n",
    "After 20 turns, the agent becomes slow, expensive, and starts forgetting earlier constraints.\n",
    "\n",
    "**What architectural change do you make to handle long conversations?**\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Medium (Tool Trust)\n",
    "\n",
    "Your agent uses an external API tool that sometimes returns incorrect data.\n",
    "\n",
    "**How do you prevent the agent from blindly trusting tool outputs?**\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Medium–Hard (Evaluation)\n",
    "\n",
    "You want to ship a new agent version safely.\n",
    "\n",
    "**What does a proper offline + online evaluation framework look like for agents?**\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Hard (Enterprise-Grade Agent)\n",
    "\n",
    "Design an enterprise agent that:\n",
    "\n",
    "* executes workflows across multiple tools\n",
    "* guarantees tenant isolation\n",
    "* supports human approval for critical actions\n",
    "* stays cost predictable at scale\n",
    "\n",
    "Explain the **core architecture and controls**.\n",
    "\n",
    "---\n",
    "\n",
    "Answer in order. I’ll grade strictly.\n"
   ],
   "id": "20b8dc26dcdf027d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
