{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Day 1",
   "id": "33ed94ec099ae4ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. **Easy** — What is the difference between *training* and *inference* in an AI system? Give one real-world example.\n",
    "\n",
    "2. **Easy–Medium** — What problem does an *embedding* solve in AI systems, and where are embeddings typically used in production?\n",
    "\n",
    "3. **Medium** — Explain how a basic **RAG (Retrieval-Augmented Generation)** pipeline works end to end. Where can it fail?\n",
    "\n",
    "4. **Medium–Hard** — You are serving an LLM-based API. Latency suddenly spikes under load. What are the *first 3 things* you would inspect, and why?\n",
    "\n",
    "5. **Hard** — You need to design a **scalable, cost-efficient AI agent system** that:\n",
    "\n",
    "   * Uses tools (APIs, DBs)\n",
    "   * Maintains short-term and long-term memory\n",
    "   * Avoids hallucinations\n",
    "\n",
    "   Describe the **architecture**, key components, and trade-offs.\n"
   ],
   "id": "66e68def430f72ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Easy\n",
    "- **Training** an AI system involves - Cleaning the data, model selection, training the model using the data so that model learns the patterns from the data.\n",
    "- Inference involves getting output from the model that has been trained.\n",
    "\n",
    "**Correct Answer**<br>\n",
    "- Training is the phase where model parameters are optimized using labeled or unlabeled data via a loss function and backpropagation.\n",
    "- Inference is the phase where the trained, frozen model generates predictions on new data with no learning involved.\n",
    "---\n",
    "2. Easy - Medi\n",
    "- Embeddings are numerical representation of data that has semantic meaning.\n",
    "- In production, Whenever a model wants to search information related to something embeddings are really helpful\n",
    "\n",
    "**Correct Answer**<br>\n",
    "- Embeddings map unstructured data into dense vectors such that semantic similarity becomes measurable using distance metrics.\n",
    "- In production, they’re used for semantic search, RAG, recommendations, deduplication, and clustering via vector databases.\n",
    "---\n",
    "3. Medium\n",
    "- RAG pipeline in short\n",
    "- Documents -> document-loader -> chunks -> embeddings -> stored in database\n",
    "- User Query -> convert into embeddings -> bi encoder compares query and documents to retrieve the relevant documents\n",
    "- retrieved documents + user query + prompt => sent to llm => llm generate response based on information and prompt.\n",
    "\n",
    "**Correct Answer**<br>\n",
    "##### **What’s missing**\n",
    "* Failure modes (explicitly asked).\n",
    "* No mention of chunk size, retrieval quality, or context window limits.\n",
    "##### **Key failures interviewers expect**\n",
    "* Poor chunking → lost context\n",
    "* Bad embeddings → irrelevant retrieval\n",
    "* Retrieval misses critical docs\n",
    "* Context overflow → truncation\n",
    "* LLM hallucination despite correct retrieval\n",
    "---\n",
    "4. Medium - Hard\n",
    "- Check the llm api calls whether they are looping or not\n",
    "- Check logs\n",
    "\n",
    "**Correct Answer**<br>\n",
    "##### Expected top 3 checks\n",
    "* LLM provider latency / rate limiting (timeouts, retries, cold starts)\n",
    "* Embedding + vector DB latency (ANN index type, cache misses)\n",
    "* Concurrency & scaling (async vs sync, worker saturation, request queuing)\n",
    "> Red flag: Not thinking in terms of systems, infrastructure, and bottlenecks.\n",
    "---\n",
    "5. Hard\n",
    "pass\n",
    "### Interview-ready answer (short, crisp, no fluff)\n",
    "\n",
    "> **Question:** Design a scalable, cost-efficient AI agent system with tools, memory, and low hallucination.\n",
    "\n",
    "**Correct Answer:**\n",
    "\n",
    "I’d design it as a **modular agent architecture**:\n",
    "\n",
    "1. **Planner (LLM-based)**\n",
    "   Interprets user intent and breaks it into steps. This reduces random tool calls and keeps execution structured.\n",
    "\n",
    "2. **Tool Executor Layer**\n",
    "   Controlled interface for APIs, databases, and services. Tools are schema-validated and permissioned to avoid unsafe or unnecessary calls.\n",
    "\n",
    "3. **Memory System (2 layers)**\n",
    "\n",
    "   * **Short-term memory:** Conversation context stored in-session (sliding window or summary).\n",
    "   * **Long-term memory:** Vector DB + metadata for facts, preferences, and past outcomes, retrieved via embeddings.\n",
    "\n",
    "4. **Retrieval + Grounding (Anti-hallucination)**\n",
    "   Use RAG for any factual queries. The agent is forced to answer *only* from retrieved context, with fallback to “I don’t know” if confidence is low.\n",
    "\n",
    "5. **Guardrails & Validation**\n",
    "\n",
    "   * Output validation (JSON schema, regex)\n",
    "   * Tool call verification\n",
    "   * Confidence thresholds on retrieval\n",
    "\n",
    "6. **Scalability & Cost Control**\n",
    "\n",
    "   * Async execution\n",
    "   * Caching embeddings and tool results\n",
    "   * Smaller models for planning, larger ones only for final response\n",
    "\n",
    "**Trade-offs:**\n",
    "More components increase complexity, but you gain reliability, lower hallucinations, and predictable costs—mandatory for production agents."
   ],
   "id": "7b13c00e42b0d71b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Day 5",
   "id": "3e910f86f917b536"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Easy\n",
    "\n",
    "**What is the difference between a *prompt* and a *system prompt* in an LLM-based application?**\n",
    "Why does this distinction matter in production?\n",
    "\n",
    "- The prompt is the instructions or query given by the user, it could be differnt for every query. Where as System prompt is defines the character/persona of the llm on how it should behave or think. It is almost same for throughout the conversation.\n",
    "- The distinction help the model to think like specific persona, system prompt is usually given by the developer that make the model think in particular way.\n"
   ],
   "id": "a8192d4ee9da6b77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Easy–Medium\n",
    "\n",
    "**Why do most production LLM systems avoid sending the entire conversation history to the model every time?**\n",
    "What strategies are used instead?\n",
    "\n",
    "- llm comes with context length which it can able to process, when the entire conversation history is sent to model everytime, the context length increase which consumes more tokens and the model's ability to reasoning decreases which causes more generalised or hallucinations.\n",
    "- TO overcome this instead of entire conversation you can send summary of conversation history that tell intent of the conversation. or Store converstation history in the vector database and using RAG pull out the messages or limit the previous chat instead of entire history only last 10 chats.\n",
    "- But among all i feel summarizing the chat history is the best option because it explicitly says the intent of the conversation."
   ],
   "id": "c06aa09fd6fea063"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Medium\n",
    "\n",
    "**You are building a customer-support AI assistant.**\n",
    "How would you prevent it from:\n",
    "\n",
    "* hallucinating answers\n",
    "* giving outdated information\n",
    "\n",
    "Explain your approach at a system level.\n",
    "\n",
    "- First identify the cause of hallucinations such as whether is it from underspecified prompt or lack of information.\n",
    "- If lack of information then i would implement RAG technique to provide relevant information to query and sent it combined to llm and explicitly give instruction to answer the query based on context.\n",
    "- I will upload the data such as FAQs, policies, resolved queries to the vector database.\n",
    "- With this method not only reduced hallucinations but also the data will be latest and up to date.\n",
    "\n",
    "---\n",
    "\n",
    "I’d ground the model using **RAG** with a trusted, up-to-date knowledge base.✅\n",
    "\n",
    "At a system level:\n",
    "\n",
    "* Retrieve relevant documents at inference time\n",
    "* Instruct the model to **answer only from retrieved context**\n",
    "* Add a fallback: *if information isn’t found, say “I don’t know”*\n",
    "\n",
    "To prevent outdated info:\n",
    "\n",
    "* Keep the knowledge base **continuously updated**\n",
    "* Version documents and monitor retrieval quality\n",
    "\n",
    "This reduces hallucinations and ensures responses stay accurate and current.\n",
    "\n",
    "---\n"
   ],
   "id": "38f276c44874eea2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Medium–Hard\n",
    "\n",
    "**An LLM agent is allowed to call tools (APIs, DBs).**\n",
    "How do you ensure:\n",
    "\n",
    "* tool calls are correct\n",
    "* the agent doesn’t loop or misuse tools\n",
    "\n",
    "What controls would you put in place?\n",
    "\n",
    "**Answer**:\n",
    "- To evaluate the tools calls are correct i would use frame works such as langsmith, deepeval. I will prepare a dataset using edgecases that would trigger the tool call. I would mention tool name and description for llm.\n",
    "- I will evaluate them based on that.\n",
    "- Then to avoid agent loop i would implement gaurdrails such as rate limiting which breaks the loop if there were many, and implement a layer which validates the tools, if there is some execution layer then i would add human in the loop mechanism.\n",
    "\n",
    "With these methods the llm agent will be robust make proper tool calls, and doesn't break.\n",
    "\n",
    "---\n",
    "\n",
    "To ensure **correct tool calls**:✅\n",
    "\n",
    "* Define strict **tool schemas and descriptions**\n",
    "* Evaluate tool usage offline with test cases and edge cases\n",
    "* Log and monitor tool-call accuracy in production\n",
    "\n",
    "To prevent **loops or misuse** at runtime:\n",
    "\n",
    "* Enforce **max tool calls per request**\n",
    "* Add **timeouts and rate limits**\n",
    "* Validate tool inputs/outputs before execution\n",
    "* Define explicit **stop conditions**\n",
    "* Use **human-in-the-loop** for high-risk actions\n",
    "\n",
    "This ensures tools are used **correctly, safely, and predictably** in production.\n",
    "\n",
    "---\n"
   ],
   "id": "6565330bd227153b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Hard\n",
    "\n",
    "**Design a multi-tenant AI platform** where:\n",
    "\n",
    "* thousands of users share the same LLM backend\n",
    "* data privacy between users is guaranteed\n",
    "* costs stay predictable\n",
    "\n",
    "Explain the **architecture, isolation strategy, and cost controls**.\n",
    "\n",
    "Here’s a **short, crisp, interview-style answer** (what a senior interviewer wants to hear):\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Hard — Multi-Tenant AI Platform (Interview Answer)**\n",
    "\n",
    "I’d use a **shared LLM backend** with a **tenant-aware application layer**.\n",
    "\n",
    "Each request carries a **tenant ID**, which is used to:\n",
    "\n",
    "* Route data to **tenant-isolated vector indexes / storage**\n",
    "* Enforce **access control and encryption** per tenant\n",
    "\n",
    "For **isolation**, I’d ensure:\n",
    "\n",
    "* Logical isolation at the data layer (separate namespaces / indexes)\n",
    "* Strict auth checks so prompts, embeddings, and logs never mix across tenants\n",
    "\n",
    "For **cost control**:\n",
    "\n",
    "* Per-tenant **rate limits and token quotas**\n",
    "* Request batching and response caching\n",
    "* Tiered plans (limits on context size, tools, and calls)\n",
    "\n",
    "This keeps inference shared and efficient, while **data privacy and costs remain predictable**."
   ],
   "id": "e33bb13cf0509c44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Day 8",
   "id": "5bd06d59b4a4366b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### 1. Easy (Real usage)\n",
    "\n",
    "You deploy an LLM-based chatbot. Users complain that answers are *sometimes correct, sometimes completely wrong*.\n",
    "**What are the first two things you would check, and why?**\n",
    "\n",
    "- I would check whether the chatbot is extracting information and telling on those based information or not, because llms hallucination rate is high when the key information to the query is missing.\n",
    "- Then if retriving informtion is not the probelm i would check the prompt and query whether they are alligned or not. Because if both query and prompt have different goal, llms gives inaccurate responses to fill the gap.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Easy–Medium (Production reality)\n",
    "\n",
    "Your RAG system works well in testing, but in production users say:\n",
    "\n",
    "> “It answers fast, but often misses obvious information from the documents.”\n",
    "\n",
    "**What are the likely causes, and how would you debug this step by step?**\n",
    "\n",
    "- The most likely cause that misses obvious information in RAG system would be llm is not utilizing the information correctly. When we retrieve relevant information using sematic search it gives approximate results it may or may not be helpful. Since there are not much step in between that's the reason it is faster.\n",
    "- I would check how documents are being stored in the vector database, chunking strategy, retrival logic. I would try to implement reranking using cross encoder for more accurate responses but there would be some latency if that's not the priority.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Medium (System behavior)\n",
    "\n",
    "An AI agent that uses tools starts repeating the same tool calls and never finishes the task.\n",
    "\n",
    "**What could cause this behavior, and what concrete safeguards would you add to stop it?**\n",
    "\n",
    "- the agent uses tool to get information if there is none then it would go again and call the tool. This happens when the parameter could be wrong, tool details like name, description could be wrong that it wouldn't understand.\n",
    "- To avoid this behaviour i would add more details about the tools so that llm can understand clearly, I would implement gaurdrails such as rate limiting if it calls the same tools again then it would stop and break the loop and say the answer with something like i don't know.\n",
    "---\n",
    "\n",
    "### 4. Medium–Hard (Scaling & cost)\n",
    "\n",
    "Your AI application suddenly gets **10× traffic** after a product launch.\n",
    "Latency and cloud costs both spike.\n",
    "\n",
    "**What immediate actions do you take, and what long-term changes do you make to the system?**\n",
    "\n",
    "- In order to reduce the load on system or handle latency i would implement load balancers, so that load is spread across different servers.\n",
    "- I would do the horizontal scaling additionally\n",
    "- To control costs - I would use cache where same or most common questions are shared across. I would per hours based cloud system. so that even though there is huge traffic my per hour cost will be the same. but it is costlier than serverless,\n",
    "---\n",
    "\n",
    "### 5. Hard (Failure under pressure)\n",
    "\n",
    "A critical enterprise customer reports that:\n",
    "\n",
    "* the AI gave a **confident but wrong answer**\n",
    "* it used **their private data in another user’s response**\n",
    "\n",
    "This is a P0 incident.\n",
    "\n",
    "**Walk me through exactly what you would do in the first hour, and what system changes you’d implement to ensure this never happens again.**\n",
    "\n",
    "- The AI always gives confident answer even the answer is factually wrong as these are predicting models. So in order to reduce the wrong answer you have to provide relevant information and mention in the prompt that use the information to answer the question.\n",
    "- Since the private data is getting leaking and to avoid that\n",
    "    - I would avoid using it plain llms response rather I would implement nodes/layers that validate the response so that any senstive is not being going response.\n",
    "    - I would implement role based check where the session info which has user's information should match the exact user's info in the response.\n",
    "- with these techniques the AI system will be more reliable and accurate.\n"
   ],
   "id": "a0c9cc89434faf25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Day 15",
   "id": "19ea43091f0edcf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Below is a **new set of 5 interview questions on fine-tuning**, **easy → hard**, written from an **applied AI engineering** perspective (production, trade-offs, failures).\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Easy\n",
    "\n",
    "**What problem does fine-tuning solve that prompting or RAG cannot?**\n",
    "Give one concrete example.\n",
    "\n",
    "> Answer:\n",
    "- Finetuning is method of post training an LLM model so that it alligns to ther user. Modifying the behaviour and tone of LLM to user centric.\n",
    "- Prompting is about giving the intructions to an LLM on how to act, respond for the query, how to output the response.\n",
    "- RAG is providing the information to the LLM where it uses that to answer the query.\n",
    "For example: In Healthcare finetune a model so that it behaves and talks like a doctor, where as RAG provide extra information if needed to answer the query. Prompt gives the instructions to the llm about its capabilities.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Easy–Medium\n",
    "\n",
    "You fine-tune an LLM and observe that:\n",
    "\n",
    "* training loss goes down\n",
    "* real user responses get worse\n",
    "\n",
    "**Why can this happen?**\n",
    "\n",
    "> Answer\n",
    "---\n",
    "\n",
    "#### 3. Medium\n",
    "\n",
    "You are deciding between **fine-tuning vs RAG vs prompt engineering** for a new product.\n",
    "\n",
    "**How do you decide which approach to use?**\n",
    "What signals push you toward fine-tuning?\n",
    "\n",
    "> Answer\n",
    "\n",
    "Identify the problem we are trying to solve, is it behaviour based or event based or actions based.\n",
    "- First off i will experiment with carefully crafted prompts to optimize the response. After maxing out prompting\n",
    "- If the model is hallucinating, response is factually incorrect then I would add information to the model using RAG.\n",
    "- If the model is giving the facts correctly but unable to perform or fullfil the task even i would go with finetuning.\n",
    "- If needed i would combine RAG to finetuned model along with prompt so that model would perform it's best to compplete the tasks.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Medium–Hard\n",
    "\n",
    "A fine-tuned model performs well in staging but fails in production with:\n",
    "\n",
    "* hallucinations\n",
    "* brittle behavior to small prompt changes\n",
    "\n",
    "**What went wrong, and how do you fix it?**\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Hard\n",
    "\n",
    "You must fine-tune an LLM for a **regulated domain** (finance / healthcare / legal) where:\n",
    "\n",
    "* training data is limited\n",
    "* mistakes are costly\n",
    "* explanations are required\n",
    "\n",
    "**Design the fine-tuning strategy, safety controls, and evaluation framework.**\n",
    "\n",
    "---\n",
    "\n",
    "Answer in order.\n",
    "I’ll evaluate like an experienced AI engineer who has shipped fine-tuned models to production.\n"
   ],
   "id": "3086f1ac475ad097"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1a30b917d42aee06"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
