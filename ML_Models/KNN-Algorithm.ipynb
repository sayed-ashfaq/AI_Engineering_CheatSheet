{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## KNN Algorithm",
   "id": "4f0ff9848662fcb0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "K-Nearest Neighbours algorithm is a supervised machine learning alogrithm generally used for classification but can also be used for Regression tasks.\n",
    "- It works by finding k closest data points to a given input\n",
    "- It makes prediction based on majority class for classification\n",
    "    - Average values for regression.\n",
    "- KNN algorithm doesn't make any assumptions about the data making it non-parametric and instanace based learning method.\n",
    "\n",
    "#### Cons\n",
    "- KNN is also called as a Lazy learner algorithm because it doesn't learn from training set immediately, instead it stores all the training data and performs computations.\n",
    "\n",
    "#### K in KNN\n",
    "- K in KNN is a number tells how many data points you should look nearby to make the prediction. More like a hyper-parameter.\n",
    "- While selection k value - if too large model become too simple and underfits.\n",
    "    - If model has lot of noise and outlier, using slightly large k values will give the good results. So choosing the optimal k value gives more accuracy.\n",
    "\n",
    "##### Statistical Methods for selection K\n",
    "- Cross validation\n",
    "- Elbow method\n",
    "- Odd values for K\n",
    "\n",
    "#### Choosing the right distance\n",
    "1. Euclidean Distance—Not ideal for High dimensional data\n",
    "- it measures the straight line distance between two points. Work well when all features are continous and similarly scaled.\n",
    "- Sensitive to large difference in feature values, Performs well on low dimensional, normalized data. Used for Geometric interpretation.\n",
    "\n",
    "2. Manhattan Distance[L1 Norm]: $(dist)=  [\\sum_{j=1}^{d} |x^{(1)}_j - x^{(2)}_j|^1]^1$\n",
    "- Computes by summing absolute differences across dimensions.\n",
    "- Useful when features represents directions, grid-based movement\n",
    "- Robust to outliers, preferred for High dimensional data and sparse feature environments.\n",
    "\n",
    "3. Minkowski Distance - $dist(x^{(1)},x^{(2)}) = \\sum_{j=1}^{d} (|x_j^{(1)}-x_j^{(2)}|^p)^{\\frac{1}{p}}$\n",
    "- Generalised version of both Manhattan(L1) and Eucledean(l2) controlled by parameter p.\n",
    "\n",
    "4. $Cosine Similarity (x^{(1)},x^{(2)}) = \\frac{x^{(1)}. \\space x^{(2)}}{||x^{(1)}|| \\space ||x^{(2)}||}$\n",
    "- Measure the angle between two vectors instead of magnitude,\n",
    "- Range (-1, 1)\n",
    "- measures angle, ignore magnitude\n",
    "- Good for text based or high dimensional data\n",
    "- Scale independent."
   ],
   "id": "d44bbdc86ce657ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T04:18:33.363937Z",
     "start_time": "2026-01-19T04:18:33.355742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## implementation of KNN from scratch\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k= 3):\n",
    "        if k <= 0:\n",
    "            raise ValueError('k must be greater than 0')\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if len(X) != len(y):\n",
    "            raise ValueError('X and y must have same length')\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def _euclidean_distance(self, x1, x2):\n",
    "        return math.sqrt(sum((a-b)**2 for a, b in zip(x1, x2)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict_single(x) for x in X]\n",
    "\n",
    "    def _predict_single(self, x):\n",
    "        distances = [\n",
    "            (self._euclidean_distance(x, x_train), y_train)\n",
    "            for x_train, y_train in zip(self.X_train, self.y_train)\n",
    "        ]\n",
    "        k_nearest = sorted(distances, key=lambda x: x[0])[:self.k]\n",
    "\n",
    "        #majority vote\n",
    "        labels = [label for _, label in k_nearest]\n",
    "        return Counter(labels).most_common(1)[0][0]\n"
   ],
   "id": "127e36d9868280d4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T04:18:33.788693Z",
     "start_time": "2026-01-19T04:18:33.779748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = [\n",
    "    [1, 2],\n",
    "    [2, 3],\n",
    "    [3, 3],\n",
    "    [6, 5],\n",
    "    [7, 7]\n",
    "]\n",
    "\n",
    "y_train = [0, 0, 0, 1, 1]\n",
    "\n",
    "knn = KNN(k=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "X_test = [\n",
    "    [2, 2],\n",
    "    [6, 6]\n",
    "]\n",
    "\n",
    "predictions = knn.predict(X_test)\n",
    "print(predictions)  # [0, 1]\n"
   ],
   "id": "4f3523b112a62c5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Curse of Dimensionality in KNN\n",
    "`What is Curse of dimensionality`\n",
    "- Curse of dimensionality is a concept that affects the model in various ways because of high dimensional data.\n",
    "- High dimensional data increases sparsity in data distribution which can result in several challenges such as increased computation, overfitting and deteriorating performance of certain algorithms.\n",
    "\n",
    "`How does it affect the kNN`:\n",
    "- Increased sparcity in data: increase in dimensions increase the volume of the space. It will be difficult to find the meaning nearest neighbours as there may be fewer within a given distance.\n",
    "- Equal distance: In high dimension the concepts of distance becomes less meaning full. The distance between points tends to become uniform and equidistance.\n",
    "- Computational Complexity: As the dimensions grows, computational complexity grows as well, as it needs to compute distances in a high dimensional space, which involves more calculations. This makes the knn slower and less efficient\n",
    "\n",
    "`How to fix the Curse of HD`:\n",
    "- There are dimensions reductions techniques by reducing the number of features while preserving the most relevant information we can use such as PCA and T-SNE."
   ],
   "id": "a2a6d5bcbf97ea57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Question about KNN algorithm\n",
    "1. **Distance Metrics and Trade-offs**\n",
    "   How do you decide between Euclidean, Manhattan, cosine, or Minkowski distance in KNN for a real-world problem, and what practical impact does this choice have on model performance and stability?\n",
    "\n",
    "2. **Curse of Dimensionality**\n",
    "   KNN often degrades in high-dimensional spaces. Explain why this happens and what concrete techniques you would apply in production to mitigate it.\n",
    "\n",
    "3. **Choosing *k* in Practice**\n",
    "   How do you select an optimal value of *k* beyond textbook cross-validation, especially when dealing with noisy or imbalanced datasets?\n",
    "\n",
    "4. **Scalability and Production Constraints**\n",
    "   KNN is considered a “lazy learner.” How would you make KNN feasible for a dataset with millions of samples and low-latency inference requirements?\n",
    "\n",
    "5. **Feature Scaling and Data Leakage**\n",
    "   Why is feature scaling critical for KNN, and how can improper scaling introduce data leakage during training and evaluation?\n"
   ],
   "id": "747c4a16b2031876"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Answers\n",
    "1. **Distance Metrics and Trade-offs**\n",
    "* Euclidean distance is good in low dimensional data, where there are less features to capture. But more prone to outliers so if the dataset contains outlier the model might not be stable.\n",
    "* Manhattan distance is ideal if the features are direction based on grid based. Robust for outliers, if dataset get more outlier then you can take over manhattan distance to make the model more stable.\n",
    "* Cosine distance calculates the angle ignoring the magnitude. It is most suitable for text based features, embeddings and so on. if magnitude is not the problem but angle is then choose this distance.\n",
    "* Minkowski if you are unsure of first 2, you can change p values and check accordingly.\n",
    "\n",
    "**Upgraded answers:** <br>\n",
    "\n",
    "Euclidean (L2) penalizes large deviations heavily, making it sensitive to outliers. Manhattan (L1) grows linearly, hence more robust. Cosine is scale-invariant and preferred for sparse/high-dimensional embeddings. Minkowski generalizes L1/L2 but is rarely tuned in practice due to interpretability and stability concerns.\n",
    "\n",
    "---\n",
    "\n",
    "2. Curse of dimensionality\n",
    "- as the no of features increases the sparsity increses where the volume of data increases.\n",
    "- it would be difficult to capture meaningful distances, also computational complexity increasese as well. This makes the model's performance to degrade.\n",
    "- To reduce the features and curse of dimensionality we can use the dimension reduction techniques such as pCA and tsne which reduces the features while preserving the information.\n",
    "- These techniques project the high dimensional data in low dimensional data.\n",
    "\n",
    "**Upgraded answers:** <br>\n",
    "\n",
    "As dimensions increase, distances concentrate, making nearest neighbors indistinguishable. KNN fails because locality is lost. In production, I reduce dimensionality using PCA or autoencoders and apply feature selection; visualization methods like t-SNE are avoided since they distort distances.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Choosing K**\n",
    "* When dealing with imbalance data you would make it balance using over sample or under sampling technique\n",
    "* Apart from cross validation you can use Elbow method and bias variance trade off to get to the optimal k value.\n",
    "\n",
    "**Upgraded answers:** <br>\n",
    "**What interviewers want** <br>\n",
    "* Small k → low bias, high variance\n",
    "* Large k → high bias, low variance\n",
    "* Metric-driven selection (F1, ROC-AUC, Recall)\n",
    "\n",
    "**Better framing** <br>\n",
    "\n",
    "I choose k by analyzing bias–variance trade-offs using task-specific metrics. For imbalanced data, I optimize recall/F1 instead of accuracy and sometimes use distance-weighted KNN to reduce majority-class dominance.\n",
    "\n",
    "---\n",
    "\n",
    "4. **Scalability and Production:** I am not sure about this\n",
    "\n",
    "**Upgraded answers:** <br>\n",
    "* You should know at least one of these:\n",
    "* KD-tree / Ball-tree (low dimensions)\n",
    "* Approximate Nearest Neighbors (ANN)\n",
    "* FAISS / HNSW / IVF\n",
    "* Vector indexing\n",
    "\n",
    "**What to say** <br>\n",
    "\n",
    "KNN is O(N) per query. To scale, I use approximate nearest neighbor search with FAISS (HNSW or IVF), reduce embedding size, and precompute indexes. For strict latency, KNN is replaced with parametric models.\n",
    "\n",
    "---\n",
    "\n",
    "5. In KNN algoirthm the assumption is data should be normalized and homogenious to calculate the distances accurately.\n",
    "- So it is mandatory to scaler the features.\n",
    "- When scaling the features you should split the data and perform the scaling on training data. if you do it combined the mean of data is shared which can also be called as data leakage.\n",
    "\n",
    "**Upgraded answers:** <br>\n",
    "Since KNN is distance-based, features must be scaled. Scalers are fit only on training data and applied to validation/test via pipelines to prevent leakage."
   ],
   "id": "5dae84155b65f17d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**What to Fix Next (High ROI)**\n",
    "\n",
    "* Learn ANN, FAISS, HNSW (non-negotiable for AI engineers)\n",
    "* Stop mentioning t-SNE outside visualization\n",
    "* Practice structured answers (Problem → Cause → Solution)\n",
    "* Tie every choice to latency, scale, or metrics"
   ],
   "id": "7652e7aa4b61ca2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e1b8ffacfdbafdd0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
