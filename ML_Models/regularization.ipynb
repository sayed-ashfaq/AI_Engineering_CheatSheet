{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Regularization in Machine Learning\n",
    "> A technique to prevent overfitting by reducing the complexity in models."
   ],
   "id": "4205575b9420c2ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ§  Phase 1: Foundations (1.5 hours)\n",
    "Goal: Be definition-proof and interviewer-safe.\n",
    "\n",
    "You must be able to say (out loud):\n",
    "* What is overfitting vs underfitting\n",
    "* What is regularization (broad definition)\n",
    "* Why deep learning needs regularization\n",
    "\n",
    "Core concepts to lock in:\n",
    "* Biasâ€“variance tradeoff\n",
    "* Model capacity\n",
    "* Generalization vs memorization\n",
    "\n",
    "Interview killer line:\n",
    "> â€œRegularization controls effective model capacity to improve generalization.â€"
   ],
   "id": "f861d9e53f51eb89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ðŸ”¥ Bias Variance Trade Off - Underfitting vs OverFitting",
   "id": "359268951db397e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Bias**: It is the difference between the average prediction of our model and the true value that we are trying to predict.**(model is too simple)**\n",
    "- Higher the bias the model tends to **underfits**\n",
    "- High error rate in both training set and test set.\n",
    "\n",
    "**Variance**: It is the measure of variability(spread) of the predicted values for a given input with the trained model. **(Model is too complex)**\n",
    "- Higher the variance mean model tends to **overfits**.\n",
    "- Low error rate on training set, larger error rate on test set.\n",
    "\n",
    "**Irreducible Error**: It represents noise in the data that can't be explained by the trained model. It always exists regardless of how good the trained model is.\n",
    "\n",
    "**Underfitting**: UF occurs when model can't capture and generalize the underlying trend of the data. It doesn't fit the data well enough as a result it will produce high errors on both training and testing data. **(High bias and low variance)**\n",
    "- The causes of Underfitting are: The lack of data to develop a model. **data size is small**\n",
    "- The underlying algorithm or model is **not capable of capturing** the patterns in the data. Using linear model to identify non linear patterns in the data.\n",
    "\n",
    "**Overfitting**: OF occurs when model fitting the training data too well and it starts the model to learn the noise of the training data.\n",
    "- Low error in training data **(low bias)** and high error in testing data **(high variance)**\n",
    "- Complicated model which has too many variables, variables that are highly correlated.\n",
    "- Over training of a model, training a model without stopping criteria.\n",
    "\n",
    "#### Trade off:\n",
    "1. if algorithm (linear regression) is too simple then it may be on high bias and low variance condition.\n",
    "2. If Algorithm (high degree equation) fits too complex then it may be on high variance and low bias.\n",
    "3. An algorithm cannot be more complex or less complex at the same time.\n",
    "\n",
    "We try to optimize the value of the total error for the model using the Bias Variance Tradeoff\n",
    "\n",
    "$ Total Error = Bias^2 + Variance + Irreducible Error$"
   ],
   "id": "9d09d225933bffe6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Regularization",
   "id": "c6c3a3887b178c77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Regularization** is a technique to **prevent overfitting by adding penalty for complexity.**\n",
    "- It implement the trade-off of the bias and variance that helps reduce the prediction error.\n",
    "\n",
    "**Model Capacity**: It is the ability of the algorithm/model to fit a wide variety of functions. The capacity of model to capture the complex patterns.\n",
    "- In neural networks it is number of nodes/paramerters(width).\n",
    "- Number of layers(depth). adding more layers increases the capacity, allowing it to learn more complex patterns.\n",
    "\n",
    "**Generalization**: The model involves identifying underlying rules, patterns and principle to make accurate predictions on unseen data. Works well both on train and test data.\n",
    "\n",
    "**Memorization**: It involve model learning the data too well and replicating the same behaviour. it doesn't really understand the underlying pattern. Works well on only train data.\n",
    "\n",
    "#### Types of Regularization\n",
    "Type 1- Modify the cost function to build different model that implement regularization.\n",
    "1. L1 (**Lasso**) Regularization\n",
    "2. L2 (**Ridge**) Regression\n",
    "3. **Elasitc Net** Regression\n",
    "\n",
    "Type 2 - **K fold Cross Validation**\n",
    "1. K Fold cross validation is technique where a model is trained multiple times on multiple samples of same data.\n",
    "\n",
    "Type 3: **Modify the ML algorithm**\n",
    "1. Change the machine learning algorithm. Use random forest instead of XGBoost to prevent overfitting."
   ],
   "id": "e03094ef2f3da636"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ“ Phase 2: Classic Regularization (2 hours)\n",
    "\n",
    "Goal: Handle math + intuition questions.\n",
    "\n",
    "Master these deeply:\n",
    "\n",
    "* L2 regularization (weight decay)\n",
    "* L1 regularization\n",
    "* Elastic Net (just conceptually)\n",
    "\n",
    "Know:\n",
    "\n",
    "* Loss function form\n",
    "* Geometric intuition\n",
    "* Why L1 â†’ sparsity\n",
    "* Why L2 â†’ smooth weights\n",
    "\n",
    "Common interview traps you must answer:\n",
    "\n",
    "* Why L2 is preferred in deep learning\n",
    "* Why L1 is rare in large neural nets\n",
    "* How weight decay differs from L2 in Adam"
   ],
   "id": "ead462a05f2784c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### L2 Regularization - Ridge Regression - add squared values of weights as penalty",
   "id": "1a2c0d624a837219"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "L2 regularization adds a **penalty proportional to the square of the weights to the loss function**. This encourages the model to keep weights small and spread importance across features rather than relying heavily on a few.\n",
    "- It discourage larger weights and reduce overfitting in deep learning and ML algorithms\n",
    "- Prevents overfitting by limiting model complxity\n",
    "- Imporves generalization to unseen data.\n",
    "- Stabilizes training and makes the model less sensitive to noise.\n",
    "\n",
    "$\n",
    "\\text{Cost} = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\lambda \\sum_{i=1}^{m} w_i^2\n",
    "$\n",
    "\n",
    "* **n**: Number of examples or data points\n",
    "* **m**: Number of features (predictor variables)\n",
    "* **yáµ¢**: Actual target value for the *i*th example\n",
    "* **Å·áµ¢**: Predicted target value for the *i*th example\n",
    "* **wáµ¢**: Coefficients of the features\n",
    "* **Î»**: Regularization parameter that controls the strength of regularization\n",
    "\n",
    "**Weight Decay** is the term heavily used in Deep Learning neural networks.\n"
   ],
   "id": "d59f4e5cb1fdca5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T05:11:12.003659Z",
     "start_time": "2026-01-31T05:11:11.988718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import make_regression #synthetic data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y =make_regression(n_samples=100, n_features=10, noise= 0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# fit the model\n",
    "ridge = Ridge(alpha= 0.1) # alpha indicate strength of penalizer\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# predictions and evaluations\n",
    "y_pred = ridge.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error: %.3f\" % mse) # lesser the better\n",
    "print(\"Coefficient of Determination:\", ridge.coef_)\n",
    "\n"
   ],
   "id": "2cc9118b7fedbe8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.145\n",
      "Coefficient of Determination: [16.79062161 54.04902485  5.17695611 63.54919093 93.46366287 70.54418812\n",
      " 86.96460871 10.38436011  3.16666288 70.77958474]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### L1 Regularization - Lasso Regression - Add absolute values for weights as penalties.",
   "id": "7a5fac7383c47185"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "L1 regularization adds a penalty to your model thatâ€™s proportional to the **absolute value** of the weights.\n",
    "\n",
    "Intuition first:\n",
    "* It discourages large weights\n",
    "* It tends to push many weights(smaller ones) exactly to **zero**\n",
    "* Result â†’ simpler, sparser models\n",
    "\n",
    "That â€œexactly zeroâ€ part is what makes L1 special.\n",
    "\n",
    "$\n",
    "\\text{Cost} = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\lambda \\sum_{i=1}^{m} |w_i|\n",
    "$\n",
    "* **n**: Number of examples or data points\n",
    "* **m**: Number of features (predictor variables)\n",
    "* **yáµ¢**: Actual target value for the *i*th example\n",
    "* **Å·áµ¢**: Predicted target value for the *i*th example\n",
    "* **wáµ¢**: Coefficients of the features\n",
    "* **Î»**: Regularization parameter that controls the strength of regularization\n",
    "\n",
    "##### When is it used in DL\n",
    "- When you feel many features are useless - Feature Selection\n",
    "- L2 is more used in Deep Learning than L1 because\n",
    "- Optimization is less smooth, slow convergence.\n",
    "- L1 selects the features; L2 distributes the importance.\n"
   ],
   "id": "cb65529b9faf9ee9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T05:39:51.100202Z",
     "start_time": "2026-01-31T05:39:51.080215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise= 0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lasso = Lasso(alpha=0.01)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lasso.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error: %.3f\" % mse)\n",
    "print(\"Coefficient of Determination:\", lasso.coef_)"
   ],
   "id": "a44cb7b6ce762f00",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.014\n",
      "Coefficient of Determination: [16.76353662 54.13124988  5.16411903 63.63537714 93.59680843 70.62844663\n",
      " 87.05844445 10.42099061  3.14811882 70.89579859]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Elastic Net Regression - Combination of L1, and L2\n",
   "id": "cb0aa4ec56587d2d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ§© Phase 3: Deep Learningâ€“Specific Regularization (2.5 hours)\n",
    "\n",
    "**Goal:** Sound like a real practitioner.\n",
    "\n",
    "### You must deeply understand:\n",
    "\n",
    "1. **Dropout**\n",
    "\n",
    "   * Training vs inference\n",
    "   * Why it works\n",
    "   * Why Transformers use less dropout now\n",
    "\n",
    "2. **Early stopping**\n",
    "\n",
    "   * Why itâ€™s implicit regularization\n",
    "   * Validation curves\n",
    "   * When it fails\n",
    "\n",
    "3. **Data augmentation**\n",
    "\n",
    "   * Why itâ€™s often stronger than weight decay\n",
    "   * Vision vs NLP vs audio differences\n",
    "\n",
    "4. **Batch Normalization**\n",
    "\n",
    "   * Why it acts as regularization\n",
    "   * Why it sometimes replaces dropout\n",
    "\n",
    "**Interview killer line:**\n",
    "\n",
    "> â€œRegularization in deep learning is often architectural, not just loss-based.â€\n"
   ],
   "id": "5b15029905cc1f65"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**DropOut** is an regularization technique where during training, it randomly deactivates (drop) a fraction of neurons in a neural network.\n",
    "- During training for each iteration we turnoff the weights randomly and let it train.\n",
    "- The rate of drop of neurons are decided by hyper parameter dropout. if `dropout = 0.5` then 50% of neurons randomly turned off for every iteration.\n",
    "- It behaves like training many different smaller networks then average them at test time. similar to ensembling.\n",
    "- It improves generalization and reduces overfitting.\n",
    "\n"
   ],
   "id": "3f0620099636270"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Revise more by opening below plan - Phase 1-3 are completed.",
   "id": "bec3d40a36b1c1b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Love this question â€” youâ€™re thinking **interview-strategically**, not just academically ðŸ‘\n",
    "Hereâ€™s the honest answer first, then a **1-day battle plan** that *actually works*.\n",
    "\n",
    "---\n",
    "\n",
    "## Is **1 day** enough to master regularization for ML / AI interviews?\n",
    "\n",
    "**Yes â€” for interviews.**\n",
    "No â€” for research-level depth (that takes weeks).\n",
    "\n",
    "In **1 focused day**, you can:\n",
    "\n",
    "* Answer **theory + intuition questions**\n",
    "* Handle **â€œwhy/when/howâ€ tradeoffs**\n",
    "* Explain **real-world usage**\n",
    "* Survive **ML engineer + AI engineer interviews**\n",
    "\n",
    "Most candidates fail because they *memorize definitions*. Youâ€™ll win by **connecting intuition â†” math â†” practice**.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ”¥ 1-DAY REGULARIZATION MASTER PLAN (8â€“10 hours)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Phase 1: Foundations (1.5 hours)\n",
    "\n",
    "**Goal:** Be definition-proof and interviewer-safe.\n",
    "\n",
    "### You must be able to say (out loud):\n",
    "\n",
    "* What is overfitting vs underfitting\n",
    "* What is regularization (broad definition)\n",
    "* Why deep learning *needs* regularization\n",
    "\n",
    "### Core concepts to lock in:\n",
    "\n",
    "* Biasâ€“variance tradeoff\n",
    "* Model capacity\n",
    "* Generalization vs memorization\n",
    "\n",
    "**Interview killer line:**\n",
    "\n",
    "> â€œRegularization controls effective model capacity to improve generalization.â€\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Phase 2: Classic Regularization (2 hours)\n",
    "\n",
    "**Goal:** Handle math + intuition questions.\n",
    "\n",
    "### Master these deeply:\n",
    "\n",
    "* **L2 regularization (weight decay)**\n",
    "* **L1 regularization**\n",
    "* Elastic Net (just conceptually)\n",
    "\n",
    "### Know:\n",
    "\n",
    "* Loss function form\n",
    "* Geometric intuition\n",
    "* Why L1 â†’ sparsity\n",
    "* Why L2 â†’ smooth weights\n",
    "\n",
    "**Common interview traps you must answer:**\n",
    "\n",
    "* Why L2 is preferred in deep learning\n",
    "* Why L1 is rare in large neural nets\n",
    "* How weight decay differs from L2 in Adam\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© Phase 3: Deep Learningâ€“Specific Regularization (2.5 hours)\n",
    "\n",
    "**Goal:** Sound like a real practitioner.\n",
    "\n",
    "![Image](https://images.openai.com/static-rsc-3/SxCkRcBo_qRfog5C0rEIWFEqkpv9OUOitxlo5bplrTHNs55h8CijHwcyJ4UtxXVbkXKB5QwWix_AMV1O6U_fag2RgOYuEdSTycmtMPIZ51k?purpose=fullsize)\n",
    "\n",
    "![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2AbnNu1lVgbpDfd9Ne0y8NkA.jpeg)\n",
    "\n",
    "![Image](https://www.researchgate.net/publication/331258691/figure/fig22/AS%3A729083155591168%401550838378112/Training-and-validation-loss-curves-for-a-network-trained-without-early-stopping-When.ppm)\n",
    "\n",
    "![Image](https://www.researchgate.net/publication/371207468/figure/fig5/AS%3A11431281164191784%401685639498540/llustration-of-the-concept-of-early-stopping-The-model-that-should-be-selected.ppm)\n",
    "\n",
    "### You must deeply understand:\n",
    "\n",
    "1. **Dropout**\n",
    "\n",
    "   * Training vs inference\n",
    "   * Why it works\n",
    "   * Why Transformers use less dropout now\n",
    "\n",
    "2. **Early stopping**\n",
    "\n",
    "   * Why itâ€™s implicit regularization\n",
    "   * Validation curves\n",
    "   * When it fails\n",
    "\n",
    "3. **Data augmentation**\n",
    "\n",
    "   * Why itâ€™s often stronger than weight decay\n",
    "   * Vision vs NLP vs audio differences\n",
    "\n",
    "4. **Batch Normalization**\n",
    "\n",
    "   * Why it acts as regularization\n",
    "   * Why it sometimes replaces dropout\n",
    "\n",
    "**Interview killer line:**\n",
    "\n",
    "> â€œRegularization in deep learning is often architectural, not just loss-based.â€\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Phase 4: Optimization & Implicit Regularization (1.5 hours)\n",
    "\n",
    "**Goal:** Beat senior-level questions.\n",
    "\n",
    "### Must-know topics:\n",
    "\n",
    "* SGD vs Adam regularization behavior\n",
    "* Why SGD generalizes better than Adam (often)\n",
    "* Sharp vs flat minima\n",
    "* Noise as regularization\n",
    "\n",
    "**If interviewer asks:**\n",
    "\n",
    "> â€œWhy does SGD generalize well even without explicit regularization?â€\n",
    "\n",
    "You should smile ðŸ˜„\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Phase 5: Practical Scenarios (1.5 hours)\n",
    "\n",
    "**Goal:** Crush system design + debugging questions.\n",
    "\n",
    "### Practice answering:\n",
    "\n",
    "* Model overfits â†’ what do you try?\n",
    "* Small dataset + big model â†’ what regularization?\n",
    "* Training loss â†“, val loss â†‘ â†’ what to do?\n",
    "* Too much regularization symptoms?\n",
    "\n",
    "### Your checklist answer should include:\n",
    "\n",
    "* Data augmentation\n",
    "* Weight decay\n",
    "* Dropout tuning\n",
    "* Early stopping\n",
    "* Model simplification\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¤ Phase 6: Interview Simulation (1 hour)\n",
    "\n",
    "**Goal:** Speak confidently, not technically mute.\n",
    "\n",
    "### Answer these aloud:\n",
    "\n",
    "1. Is dropout regularization?\n",
    "2. Is early stopping regularization?\n",
    "3. L1 vs L2 â€” when and why?\n",
    "4. Why not just collect more data?\n",
    "5. Regularization in Transformers vs CNNs\n",
    "6. Can regularization hurt performance?\n",
    "\n",
    "If you can explain to a **non-ML person**, youâ€™re ready.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ§  Cheat Sheet (memorize this)\n",
    "\n",
    "> Regularization = anything that improves generalization by limiting effective model complexity\n",
    "\n",
    "* L1/L2 â†’ explicit\n",
    "* Dropout â†’ explicit stochastic\n",
    "* Data augmentation â†’ data-level\n",
    "* Early stopping â†’ implicit\n",
    "* BatchNorm â†’ implicit\n",
    "* SGD noise â†’ implicit\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ After this 1 day, you will:\n",
    "\n",
    "âœ… Pass ML engineer interviews\n",
    "âœ… Sound senior in AI engineer interviews\n",
    "âœ… Explain tradeoffs, not just definitions\n",
    "âœ… Handle follow-ups calmly\n",
    "\n",
    "---\n",
    "\n",
    "If you want, next I can:\n",
    "\n",
    "* Give **20 real interview questions + ideal answers**\n",
    "* Create a **1-page interview cheat sheet**\n",
    "* Run a **mock interview** with you\n",
    "* Convert this into a **Notion / PDF study plan**\n",
    "\n",
    "Just tell me how you want to practice ðŸ’ª\n"
   ],
   "id": "940e190428e586c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "67cab0b6efe42b0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
